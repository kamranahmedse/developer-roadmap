{
  "MEL6y3vwiqwAV6FQihF34": {
    "title": "Introduction",
    "description": "Machine learning is about creating computer programs that can learn from data. Instead of being explicitly programmed to perform a task, these programs improve their performance on a specific task as they are exposed to more data. This learning process allows them to make predictions or decisions without being directly told how to do so.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Machine Learning?",
        "url": "https://www.ibm.com/topics/machine-learning",
        "type": "article"
      },
      {
        "title": "What is Machine Learning?",
        "url": "https://www.youtube.com/watch?v=9gGnTQTYNaE",
        "type": "video"
      }
    ]
  },
  "GHO6lN3GTiIRH1P70IRaZ": {
    "title": "ML Engineer vs AI Engineer",
    "description": "An ML Engineer primarily concentrates on building, deploying, and maintaining machine learning models in production environments. An AI Engineer, on the other hand, typically has a broader scope, encompassing the design and development of entire AI systems, which may include components beyond just machine learning, such as natural language processing, computer vision, and robotics.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "AI VS ML Engineer What Do They Do?",
        "url": "https://www.youtube.com/watch?v=Ff8HHBITvfs",
        "type": "video"
      }
    ]
  },
  "BzZd-d5t63dY97SRSIb0J": {
    "title": "Skills and Responsibilities",
    "description": "Machine learning roles require a blend of technical expertise and practical abilities. These roles involve designing, developing, and deploying machine learning models to solve real-world problems. Key skills include proficiency in programming languages like Python, a strong understanding of statistical concepts, and experience with machine learning frameworks. Responsibilities often encompass data collection and preprocessing, model selection and training, performance evaluation, and continuous model improvement.",
    "links": []
  },
  "FgzPlLUfGdlZPvPku0-Xl": {
    "title": "What is an ML Engineer?",
    "description": "An ML Engineer focuses on building, deploying, and maintaining machine learning systems in production. They bridge the gap between data science and software engineering, taking models developed by data scientists and making them scalable, reliable, and efficient for real-world applications. This involves tasks like data pipeline construction, model deployment, performance monitoring, and infrastructure management.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What Is a Machine Learning Engineer? (+ How to Get Started)",
        "url": "https://www.coursera.org/articles/what-is-machine-learning-engineer",
        "type": "article"
      }
    ]
  },
  "83UDoO1vC0LjL-qpI0Jh-": {
    "title": "Linear Algebra",
    "description": "Linear algebra is a branch of mathematics that deals with vector spaces and linear transformations between those spaces. It involves concepts like vectors, matrices, and systems of linear equations, and provides tools for manipulating and solving problems involving these entities. Operations such as matrix multiplication, decomposition, and eigenvalue analysis are fundamental to this field.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Linear algebra for data science",
        "url": "http://mitran-lab.amath.unc.edu/courses/MATH347DS/textbook.pdf",
        "type": "article"
      },
      {
        "title": "How I learned Linear Algebra, Probability and Statistics for Data Science",
        "url": "https://towardsdatascience.com/how-i-learned-linear-algebra-probability-and-statistics-for-data-science-b9d1c34dfa56/",
        "type": "article"
      },
      {
        "title": "Linear Algebra for Machine Learning",
        "url": "https://www.youtube.com/watch?v=QCPJ0VdpM00",
        "type": "video"
      }
    ]
  },
  "wHlEinHuRARp5OfSulpA-": {
    "title": "Calculus",
    "description": "Calculus is a branch of mathematics that deals with continuous change. It provides tools and techniques for understanding rates of change and accumulation. The two main branches are differential calculus, which focuses on finding the rate of change of a function, and integral calculus, which focuses on finding the accumulation of quantities. These concepts are fundamental for optimization, modeling, and understanding the behavior of functions.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Calculus Online Textbook",
        "url": "https://ocw.mit.edu/courses/res-18-001-calculus-fall-2023/pages/textbook/",
        "type": "article"
      },
      {
        "title": "Calculus",
        "url": "https://en.wikipedia.org/wiki/Calculus",
        "type": "article"
      },
      {
        "title": "Calculus",
        "url": "https://www.youtube.com/playlist?list=PLybg94GvOJ9ELZEe9s2NXTKr41Yedbw7M",
        "type": "video"
      }
    ]
  },
  "jJukG4XxfFcID_VlQKqe-": {
    "title": "Chain rule of derivation",
    "description": "The chain rule is a formula for finding the derivative of a composite function. If you have a function that's made up of one function inside another (like sin(xÂ²) ), the chain rule lets you break down the differentiation process. It states that the derivative of the composite function is the derivative of the outer function evaluated at the inner function, multiplied by the derivative of the inner function.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Chain rule",
        "url": "https://en.wikipedia.org/wiki/Chain_rule",
        "type": "article"
      },
      {
        "title": "Derivatives of Composite Functions: The Chain Rule",
        "url": "https://www.youtube.com/watch?v=_x1nCg2LfuA",
        "type": "video"
      }
    ]
  },
  "3BxbkrBp8veZj38zdwN8s": {
    "title": "Gradient, Jacobian, Hessian",
    "description": "The gradient, Jacobian, and Hessian are fundamental tools from calculus used to analyze and optimize functions, especially in the context of machine learning. The gradient of a scalar-valued function of multiple variables is a vector containing the partial derivatives with respect to each variable, indicating the direction of the steepest ascent. The Jacobian matrix generalizes the gradient to vector-valued functions of multiple variables, containing all the partial derivatives of each output component with respect to each input variable. The Hessian matrix, on the other hand, is the square matrix of second-order partial derivatives of a scalar-valued function, providing information about the local curvature of the function.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Vector Calculus: Understanding the Gradient",
        "url": "https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/",
        "type": "article"
      },
      {
        "title": "A Gentle Introduction to the Jacobian",
        "url": "https://machinelearningmastery.com/a-gentle-introduction-to-the-jacobian/",
        "type": "article"
      },
      {
        "title": "A Gentle Introduction To Hessian Matrices",
        "url": "https://www.machinelearningmastery.com/a-gentle-introduction-to-hessian-matrices/",
        "type": "article"
      },
      {
        "title": "Partial Derivatives and the Gradient of a Function",
        "url": "https://www.youtube.com/watch?v=AXH9Xm6Rbfc&t=320s&pp=ygURZ3JhZGllbnQgY2FsY3VsdXM%3D",
        "type": "video"
      },
      {
        "title": "Change of Variables and the Jacobian",
        "url": "https://www.youtube.com/watch?v=hhFzJvaY__U",
        "type": "video"
      },
      {
        "title": "Multivariable Calculus: Lecture 3 Hessian Matrix : Optimization for a three variable function",
        "url": "https://www.youtube.com/watch?v=zomvvohLwr4",
        "type": "video"
      }
    ]
  },
  "GN6SnI7RXIeW8JeD-qORW": {
    "title": "Derivatives, Partial Derivatives",
    "description": "A derivative measures how a function changes as its input changes. Imagine a curve on a graph; the derivative at a specific point tells you the slope of the line tangent to that curve at that point. When dealing with functions of multiple variables, we use partial derivatives. A partial derivative measures how a function changes with respect to one specific variable, while holding all other variables constant.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Understanding Derivatives: A Comprehensive Guide to Their Uses and Benefits",
        "url": "https://www.investopedia.com/terms/d/derivative.asp",
        "type": "article"
      },
      {
        "title": "Derivatives",
        "url": "https://en.wikipedia.org/wiki/Derivative",
        "type": "article"
      },
      {
        "title": "What is a Derivative? Deriving the Power Rule",
        "url": "https://www.youtube.com/watch?v=x3iEEDxrhyE",
        "type": "video"
      }
    ]
  },
  "d7J8GEkut61NDGRzROJoP": {
    "title": "Scalars, Vectors, Tensors",
    "description": "Scalars, vectors, and tensors are fundamental building blocks for representing data in machine learning. A scalar is a single numerical value, like a temperature reading. A vector is an ordered array of numbers, representing a point in space or a feature set for a single data instance. A tensor is a generalization of vectors and matrices to higher dimensions; it can be thought of as a multi-dimensional array, useful for representing images, videos, or complex datasets with multiple features and relationships.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "From Vectors to Tensors: Exploring the Mathematics of Tensor Algebra",
        "url": "https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff/",
        "type": "article"
      },
      {
        "title": "Scalar, Vector, Tensor",
        "url": "https://e-magnetica.pl/doku.php/scalar_vector_tensor",
        "type": "article"
      },
      {
        "title": "What the HECK is a Tensor?!?",
        "url": "https://www.youtube.com/watch?v=bpG3gqDM80w",
        "type": "video"
      }
    ]
  },
  "yGs2h10gZcO4GMaWfI3uW": {
    "title": "Singular Value Decomposition",
    "description": "Singular Value Decomposition (SVD) is a matrix factorization technique that decomposes a rectangular matrix into three other matrices: a unitary matrix, a diagonal matrix of singular values, and another unitary matrix. This decomposition reveals the underlying structure of the original matrix, highlighting its principal components and allowing for dimensionality reduction and noise removal. Essentially, SVD breaks down a complex matrix into simpler, more manageable components that capture the most important information.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Singular Value Decomposition",
        "url": "https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf",
        "type": "article"
      },
      {
        "title": "Singular Value Decomposition",
        "url": "https://en.wikipedia.org/wiki/Singular_value_decomposition",
        "type": "article"
      },
      {
        "title": "Singular Value Decomposition (SVD): Overview",
        "url": "https://www.youtube.com/watch?v=gXbThCXjZFM",
        "type": "video"
      }
    ]
  },
  "1IhaXJxNREq2HA1nT-lMM": {
    "title": "Matrix & Matrix Operations",
    "description": "A matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. Matrix operations are the rules and procedures for manipulating these matrices. These operations include addition, subtraction, multiplication, transposition (flipping rows and columns), and finding the inverse of a matrix, each with specific rules about the dimensions of the matrices involved.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Matrix (mathematics)",
        "url": "https://en.wikipedia.org/wiki/Matrix_(mathematics)",
        "type": "article"
      },
      {
        "title": "Linear Algebra - Matrix Operations",
        "url": "https://www.youtube.com/watch?v=p48uw2vFWQs",
        "type": "article"
      }
    ]
  },
  "3p98Uwf8gyALDr-89lBEZ": {
    "title": "Eigenvalues, Diagonalization",
    "description": "Eigenvalues are special numbers associated with a square matrix that, when multiplied by a corresponding eigenvector, result in the same vector scaled by that eigenvalue. Diagonalization is the process of transforming a square matrix into a diagonal matrix, where all off-diagonal elements are zero, using its eigenvectors and eigenvalues. This transformation simplifies many matrix operations and provides insights into the matrix's properties.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Eigenvalues and eigenvectors",
        "url": "https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors",
        "type": "article"
      },
      {
        "title": "Matrix Diagonalization",
        "url": "https://www.statlect.com/matrix-algebra/matrix-diagonalization",
        "type": "article"
      },
      {
        "title": "Finding Eigenvalues and Eigenvectors",
        "url": "https://www.youtube.com/watch?v=TQvxWaQnrqI",
        "type": "video"
      },
      {
        "title": "Diagonalization",
        "url": "https://www.youtube.com/watch?v=WTLl03D4TNA",
        "type": "video"
      }
    ]
  },
  "XmnWnPE1sVXheuc-M_Ew7": {
    "title": "Determinants, inverse of Matrix",
    "description": "A determinant is a scalar value that can be computed from the elements of a square matrix and encodes certain properties of the linear transformation described by the matrix. The inverse of a matrix, denoted as Aâ»Â¹, is another matrix that, when multiplied by the original matrix A, results in the identity matrix. The inverse exists only for square matrices with a non-zero determinant, making the matrix invertible.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Determinant of a Matrix",
        "url": "https://www.mathsisfun.com/algebra/matrix-determinant.html",
        "type": "article"
      },
      {
        "title": "Inverse of a Matrix",
        "url": "https://www.mathsisfun.com/algebra/matrix-inverse.html",
        "type": "article"
      },
      {
        "title": "Determinant of a Matrix",
        "url": "https://www.youtube.com/watch?v=CcbyMH3Noow",
        "type": "video"
      },
      {
        "title": "Inverse Matrices and Their Properties",
        "url": "https://www.youtube.com/watch?v=kWorj5BBy9k",
        "type": "video"
      }
    ]
  },
  "5DiaZkljhHAGPi9DkaH3b": {
    "title": "Statistics",
    "description": "Statistics is the science of collecting, analyzing, interpreting, presenting, and organizing data. It is a branch of mathematics that deals with the collection, analysis, interpretation, presentation, and organization of data. It is used in a wide range of fields, including science, engineering, medicine, and social science. Statistics is used to make informed decisions, to predict future events, and to test hypotheses. It is also used to summarize data, to describe relationships between variables, and to make inferences about populations based on samples.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Introductory Statistics",
        "url": "https://assets.openstax.org/oscms-prodcms/media/documents/IntroductoryStatistics-OP_i6tAI7e.pdf",
        "type": "article"
      },
      {
        "title": "Introduction to Statistics",
        "url": "https://imp.i384100.net/3eRv4v",
        "type": "article"
      },
      {
        "title": "Statistics - A Full University Course on Data Science Basics",
        "url": "https://www.youtube.com/watch?v=xxpc-HPKN28",
        "type": "video"
      }
    ]
  },
  "p8q1Gtt9x19jw5_-YjAGh": {
    "title": "Basics of Probability",
    "description": "Probability is a way to quantify the likelihood of an event occurring. It provides a numerical measure, ranging from 0 to 1, representing the chance that a specific outcome will happen. A probability of 0 indicates impossibility, while a probability of 1 signifies certainty. Understanding probability involves concepts like random variables, probability distributions, and events, which are essential for making predictions and decisions under uncertainty.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Probability and Statistics: The Science of Uncertainty",
        "url": "https://utstat.utoronto.ca/mikevans/jeffrosenthal/book.pdf",
        "type": "article"
      },
      {
        "title": "Probability",
        "url": "https://en.wikipedia.org/wiki/Probability",
        "type": "article"
      },
      {
        "title": "Probability",
        "url": "https://www.mathsisfun.com/data/probability.html",
        "type": "article"
      },
      {
        "title": "Probability Bootcamp",
        "url": "https://www.youtube.com/playlist?list=PLMrJAkhIeNNR3sNYvfgiKgcStwuPSts9V",
        "type": "video"
      }
    ]
  },
  "ZaoZ2XxicKuTDn4uxe52L": {
    "title": "Descriptive Statistics",
    "description": "Descriptive statistics involves methods for summarizing and organizing data in a meaningful way. It focuses on describing the main features of a dataset using measures like mean, median, mode, standard deviation, and range. These techniques help to understand the central tendency, variability, and distribution of the data without making inferences beyond the specific dataset.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Descriptive Statistics: Definition, Overview, Types, and Examples",
        "url": "https://www.investopedia.com/terms/d/descriptive_statistics.asp",
        "type": "article"
      },
      {
        "title": "Descriptive Statistics | Definitions, Types, Examples",
        "url": "https://www.scribbr.com/statistics/descriptive-statistics/",
        "type": "article"
      }
    ]
  },
  "P32Rmnln5NCFWz4LP0k05": {
    "title": "Basic concepts",
    "description": "Statistics is the science of collecting, analyzing, interpreting, and presenting data. It provides the foundation for understanding patterns and making inferences from data, which is crucial for machine learning algorithms. Here are 10 basic statistical concepts:\n\n*   **Mean:** The average value of a dataset, calculated by summing all values and dividing by the number of values.\n*   **Median:** The middle value in a sorted dataset.\n*   **Mode:** The value that appears most frequently in a dataset.\n*   **Standard Deviation:** A measure of the spread or dispersion of data points around the mean.\n*   **Variance:** The square of the standard deviation, representing the average squared difference from the mean.\n*   **Probability:** The likelihood of an event occurring, expressed as a number between 0 and 1.\n*   **Distributions:** A function that shows the possible values for a variable and how often they occur (e.g., normal distribution, uniform distribution).\n*   **Hypothesis Testing:** A method for testing a claim or hypothesis about a population based on a sample of data.\n*   **Correlation:** A statistical measure that describes the extent to which two variables are related.\n*   **Regression:** A statistical method for modeling the relationship between a dependent variable and one or more independent variables.",
    "links": []
  },
  "WLlZE_vto-CYY5GLV_w7o": {
    "title": "Types of Distribution",
    "description": "A distribution describes how data is spread or arranged across its possible values. It provides a way to understand the probability of different outcomes occurring within a dataset. Different types of distributions, like normal, uniform, or binomial, each have unique characteristics that determine the likelihood of observing specific values. Understanding these distributions is essential for summarizing data, making predictions, and performing statistical inference.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Probability Distribution | Formula, Types, & Examples",
        "url": "https://www.scribbr.com/statistics/probability-distributions/",
        "type": "article"
      },
      {
        "title": "List of probability distributions",
        "url": "https://en.wikipedia.org/wiki/List_of_probability_distributions",
        "type": "article"
      },
      {
        "title": "Probability: Types of Distributions",
        "url": "https://www.youtube.com/watch?v=b9a27XN_6tg",
        "type": "video"
      }
    ]
  },
  "P576TdYcbE6v3RpJntiKw": {
    "title": "Random Variances, PDFs",
    "description": "A random variable is a variable whose value is a numerical outcome of a random phenomenon. It can be discrete (taking on a finite or countably infinite number of values) or continuous (taking on any value within a given range).\n\nThe probability density function (PDF) describes the relative likelihood for a continuous random variable to take on a given value. It's important to note that the value of the PDF at any given point is not a probability itself, but rather the area under the PDF curve over a given interval represents the probability of the random variable falling within that interval.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Random Variable: What is it in Statistics?",
        "url": "https://www.statisticshowto.com/random-variable/",
        "type": "article"
      },
      {
        "title": "The Basics of Probability Density Function (PDF), With an Example",
        "url": "https://www.investopedia.com/terms/p/pdf.asp",
        "type": "article"
      },
      {
        "title": "Sample Variance in Random Population Sampling",
        "url": "https://www.youtube.com/watch?v=yNnUVHfX5yQ",
        "type": "video"
      }
    ]
  },
  "7o6g0wQxHH9i9MMCoDq2C": {
    "title": "Bayes Theorem",
    "description": "Bayes' Theorem is a mathematical formula that describes how to update the probability of a hypothesis based on new evidence. It essentially calculates the probability of an event occurring given that another event has already occurred. The theorem uses prior knowledge of conditions related to the event to refine the probability estimate as new information becomes available.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Bayes' Theorem: What It Is, Formula, and Examples",
        "url": "https://www.investopedia.com/terms/b/bayes-theorem.asp",
        "type": "article"
      },
      {
        "title": "Bayes' Theorem",
        "url": "https://www.mathsisfun.com/data/bayes-theorem.html",
        "type": "article"
      },
      {
        "title": "Bayes' Theorem EXPLAINED with Examples",
        "url": "https://www.youtube.com/watch?v=cqTwHnNbc8g",
        "type": "video"
      }
    ]
  },
  "DUIrJwuYHlhJvZJT2acaY": {
    "title": "Inferential Statistics",
    "description": "Inferential statistics uses sample data to make inferences or predictions about a larger population. Instead of examining the entire population, which is often impractical or impossible, we analyze a representative subset (the sample) and then use statistical methods to draw conclusions about the characteristics of the whole population. This involves estimating population parameters (like the mean or proportion) and testing hypotheses about these parameters based on the sample data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Inferential Statistics | An Easy Introduction & Examples",
        "url": "https://www.scribbr.com/statistics/inferential-statistics/",
        "type": "article"
      }
    ]
  },
  "MYZUJ1uHIaRd1Gb4ORzwG": {
    "title": "Graphs & Charts",
    "description": "Graphs and charts are visual representations of data. They use symbols like bars, lines, and slices to display patterns, trends, and relationships within datasets. These visual tools help in understanding complex information quickly and making data more accessible and interpretable.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Data Visualization?",
        "url": "https://www.ibm.com/think/topics/data-visualization",
        "type": "article"
      }
    ]
  },
  "tP0oBkjvJC9hrtARkgLon": {
    "title": "Probability",
    "description": "Linear algebra is a branch of mathematics that deals with vector spaces and linear transformations between those spaces. It involves concepts like vectors, matrices, and systems of linear equations, and provides tools for manipulating and solving problems involving these entities. Operations such as matrix multiplication, decomposition, and eigenvalue analysis are fundamental to this field.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Linear algebra for data science",
        "url": "http://mitran-lab.amath.unc.edu/courses/MATH347DS/textbook.pdf",
        "type": "article"
      },
      {
        "title": "How I learned Linear Algebra, Probability and Statistics for Data Science",
        "url": "https://towardsdatascience.com/how-i-learned-linear-algebra-probability-and-statistics-for-data-science-b9d1c34dfa56/",
        "type": "article"
      },
      {
        "title": "Linear Algebra for Machine Learning",
        "url": "https://www.youtube.com/watch?v=QCPJ0VdpM00",
        "type": "video"
      }
    ]
  },
  "N_vLjBVdsGsoePtqlqh2w": {
    "title": "Discrete Mathematics",
    "description": "Discrete mathematics deals with mathematical structures that are fundamentally discrete rather than continuous. This means it focuses on objects that have distinct, separated values, like integers, graphs, and logical statements. It provides the theoretical foundations and tools for reasoning about and modeling these discrete structures. This field is essential for computer science, as it provides the foundation for understanding algorithms, data structures, and information networks.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Discrete Mathematics",
        "url": "https://en.wikipedia.org/wiki/Discrete_mathematics",
        "type": "article"
      },
      {
        "title": "Discrete Math (Full Course: Sets, Logic, Proofs, Probability, Graph Theory, etc)",
        "url": "https://www.youtube.com/playlist?list=PLHXZ9OQGMqxersk8fUxiUMSIx0DBqsKZS",
        "type": "article"
      }
    ]
  },
  "qDn1elMoPIBgQSCWiYkLI": {
    "title": "Python",
    "description": "Python is an interpreted high-level general-purpose programming language. Its design philosophy emphasizes code readability with its significant use of indentation. Its language constructs as well as its object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming. Python is often described as a \"batteries included\" language due to its comprehensive standard library.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Visit the Dedicated Python Developer Roadmap",
        "url": "https://roadmap.sh/python",
        "type": "article"
      },
      {
        "title": "Python Website",
        "url": "https://www.python.org/",
        "type": "article"
      },
      {
        "title": "Python - Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
        "type": "article"
      },
      {
        "title": "Tutorial Series: How to Code in Python",
        "url": "https://www.digitalocean.com/community/tutorials/how-to-write-your-first-python-3-program",
        "type": "article"
      },
      {
        "title": "Google's Python Class",
        "url": "https://developers.google.com/edu/python",
        "type": "article"
      },
      {
        "title": "Learn Python - Full Course",
        "url": "https://www.youtube.com/watch?v=4M87qBgpafk",
        "type": "video"
      }
    ]
  },
  "hWA7RtuqltMTmHdcCnmES": {
    "title": "Basic Syntax",
    "description": "Python's basic syntax defines the rules for writing code that the interpreter can understand and execute. This includes how to structure lines of code, use indentation to define code blocks, write comments for explanation, assign values to variables, and perform basic operations using operators. Understanding these fundamental elements is essential for writing any Python program, including those used in machine learning.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "isit Dedicated Python Roadmap",
        "url": "https://roadmap.sh/python",
        "type": "article"
      },
      {
        "title": "Learn Python - Full Course",
        "url": "https://www.youtube.com/watch?v=4M87qBgpafk",
        "type": "video"
      }
    ]
  },
  "dEFLBGpiH6nbSMeR7ecaT": {
    "title": "Variables and Data Types",
    "description": "Variables are named storage locations in a computer's memory used to hold data. Data types classify the kind of value a variable can hold, such as numbers (integers, decimals), text (strings), or boolean values (true/false). Understanding variables and data types is fundamental to writing any program, as it dictates how data is stored, manipulated, and used within the code.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Variables in Python",
        "url": "https://realpython.com/python-variables",
        "type": "article"
      },
      {
        "title": "Python for Beginners: Data Types",
        "url": "https://thenewstack.io/python-for-beginners-data-types/",
        "type": "article"
      },
      {
        "title": "Python Variables and Data Types",
        "url": "https://www.youtube.com/playlist?list=PLBlnK6fEyqRhN-sfWgCU1z_Qhakc1AGOn",
        "type": "video"
      }
    ]
  },
  "NP1kjSk0ujU0Gx-ajNHlR": {
    "title": "Conditionals",
    "description": "Conditional statements in Python allow you to execute different blocks of code based on whether a certain condition is true or false. The most common conditional statements are `if`, `elif` (else if), and `else`. An `if` statement checks a condition, and if it's true, the code block under it runs. `elif` allows you to check additional conditions if the initial `if` condition is false. Finally, `else` provides a block of code to execute if none of the preceding `if` or `elif` conditions are true. These statements enable programs to make decisions and respond differently to various inputs or situations.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Conditional Statements in Python",
        "url": "https://realpython.com/python-conditional-statements/",
        "type": "article"
      },
      {
        "title": "Learn Python CONDITIONAL EXPRESSIONS in 5 minutes!",
        "url": "https://www.youtube.com/watch?v=TYyKQBC4bwE",
        "type": "video"
      }
    ]
  },
  "R9DQNc0AyAQ2HLpP4HOk6": {
    "title": "Data Structures",
    "description": "Python provides several built-in data structures for organizing and storing data. These structures include lists, which are ordered and mutable collections; tuples, which are ordered and immutable collections; dictionaries, which store data in key-value pairs; and sets, which are unordered collections of unique elements. Each data structure offers different performance characteristics and is suitable for various tasks depending on the specific requirements of the program.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Data Structures",
        "url": "https://docs.python.org/3/tutorial/datastructures.html",
        "type": "article"
      },
      {
        "title": "Data Structures and Algorithms in Python - Full Course for Beginners",
        "url": "https://www.youtube.com/watch?v=pkYVOmU3MgA",
        "type": "video"
      }
    ]
  },
  "fNTb9y3zs1HPYclAmu_Wv": {
    "title": "Exceptions",
    "description": "Exceptions in Python are events that disrupt the normal flow of a program's execution. They occur when the interpreter encounters an error during runtime, such as trying to divide by zero or accessing an index that's out of bounds in a list. When an exception occurs, Python creates an exception object. If the exception isn't handled, the program will terminate and display an error message. However, you can use `try` and `except` blocks to catch and handle exceptions, allowing your program to continue running even when errors occur.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Errors and Exceptions",
        "url": "https://docs.python.org/3/tutorial/errors.html",
        "type": "article"
      },
      {
        "title": "Python Exceptions: An Introduction",
        "url": "https://realpython.com/python-exceptions/",
        "type": "article"
      },
      {
        "title": "Learn Python EXCEPTION HANDLING in 5 minutes!",
        "url": "https://www.youtube.com/watch?v=V_NXT2-QIlE",
        "type": "video"
      }
    ]
  },
  "-DJgS6l2qngfwurExlmmT": {
    "title": "Functions, Builtin Functions",
    "description": "Functions are reusable blocks of code that perform a specific task. They take inputs, process them, and return an output. Python provides many built-in functions like `print()` for displaying output, `len()` for finding the length of a sequence, and `type()` for determining the data type of a variable. These built-in functions are readily available for use, while you can also define your own custom functions, also known as user-defined functions (UDFs), to encapsulate specific logic and improve code organization.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Built-in Functions",
        "url": "https://docs.python.org/3/library/functions.html",
        "type": "article"
      },
      {
        "title": "Python Functions: How to Call & Write Functions",
        "url": "https://www.datacamp.com/tutorial/functions-python-tutorial",
        "type": "article"
      },
      {
        "title": "Functions in Python | Python for Beginners",
        "url": "https://www.youtube.com/watch?v=zvzjaqMBEso",
        "type": "video"
      }
    ]
  },
  "Dvy7BnNzK55qbh_SgOk8m": {
    "title": "Loops",
    "description": "Loops in Python are a way to repeat a block of code multiple times. They allow you to execute a set of instructions over and over, either a specific number of times or until a certain condition is met. Python has two main types of loops: `for` loops, which are typically used to iterate over a sequence (like a list or string), and `while` loops, which continue executing as long as a given condition remains true.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Loops",
        "url": "https://www.learnpython.org/en/Loops",
        "type": "article"
      },
      {
        "title": "Python While Loops & For Loops | Python tutorial for Beginners",
        "url": "https://www.youtube.com/watch?v=23vCap6iYSs",
        "type": "article"
      }
    ]
  },
  "nN9BumBHi-c9HKFlgL2GH": {
    "title": "Object Oriented Programming",
    "description": "In Python, object-oriented Programming (OOPs) is a programming paradigm that uses objects and classes in programming. It aims to implement real-world entities like inheritance, polymorphism, encapsulation, etc., in programming. The main concept of OOPs is to bind the data and the functions that work on that together as a single unit so that no other part of the code can access this data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Object Oriented Programming in Python",
        "url": "https://realpython.com/python3-object-oriented-programming/",
        "type": "article"
      },
      {
        "title": "Object Oriented Programming with Python - Full Course for Beginners",
        "url": "https://www.youtube.com/watch?v=Ej_02ICOIgs",
        "type": "video"
      },
      {
        "title": "Object Oriented Programming (OOP) In Python - Beginner Crash Course",
        "url": "https://www.youtube.com/watch?v=-pEs-Bss8Wc/",
        "type": "video"
      },
      {
        "title": "Python OOP Tutorial",
        "url": "https://www.youtube.com/watch?v=IbMDCwVm63M",
        "type": "video"
      }
    ]
  },
  "ybqwtlHG4HMm5lyUKW2SO": {
    "title": "Essential libraries",
    "description": "Python's popularity in data analysis stems from its rich ecosystem of specialized libraries. **NumPy** provides powerful tools for numerical computation, particularly with arrays and matrices. **Pandas** offers data structures like DataFrames, making it easy to organize, manipulate, and analyze tabular data. For visualization, **Matplotlib** is a fundamental library for creating static, interactive, and animated plots. Building on Matplotlib, **Seaborn** provides a high-level interface for drawing attractive and informative statistical graphics. These libraries collectively enable efficient data handling, exploration, and presentation.",
    "links": []
  },
  "nKE9sO-f2fdMiuLu2xby1": {
    "title": "Numpy",
    "description": "NumPy is a fundamental Python library used for numerical computing. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently. These arrays are homogeneous, meaning they contain elements of the same data type, which allows for optimized storage and computation. NumPy is widely used in data science, machine learning, and scientific computing due to its performance and ease of use.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "NumPy",
        "url": "https://github.com/numpy/numpy",
        "type": "opensource"
      },
      {
        "title": "NumPy",
        "url": "https://numpy.org/",
        "type": "article"
      },
      {
        "title": "Python NumPy Array Tutorial",
        "url": "https://www.datacamp.com/tutorial/python-numpy-tutorial",
        "type": "article"
      },
      {
        "title": "Learn NumPy in 1 hour!",
        "url": "https://www.youtube.com/watch?v=VXU4LSAQDSc",
        "type": "video"
      }
    ]
  },
  "PnOoShqB3z4LuUvp0Gh2e": {
    "title": "Pandas",
    "description": "Pandas is a library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "pandas - Python Data Analysis Library",
        "url": "https://pandas.pydata.org/",
        "type": "article"
      },
      {
        "title": "Complete Python Pandas Data Science Tutorial! (2025 Updated Edition)",
        "url": "https://www.youtube.com/watch?v=2uvysYbKdjM",
        "type": "video"
      }
    ]
  },
  "OXbATvlhBXTQ1iRGwPUfb": {
    "title": "Matplotlib",
    "description": "Matplotlib is a paramount data visualization library used extensively by data analysts for generating a wide array of plots and graphs. Through Matplotlib, data analysts can convey results clearly and effectively, driving insights from complex data sets. It offers a hierarchical environment which is very natural for a data scientist to work with. Providing an object-oriented API, it allows for extensive customization and integration into larger applications. From histograms, bar charts, scatter plots to 3D graphs, the versatility of Matplotlib assists data analysts in the better comprehension and compelling representation of data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Matplotlib",
        "url": "https://matplotlib.org/",
        "type": "article"
      },
      {
        "title": "Learn Matplotlib in 6 minutes",
        "url": "https://www.youtube.com/watch?v=nzKy9GY12yo",
        "type": "video"
      }
    ]
  },
  "VYVLUxhp3XxxknNr5V966": {
    "title": "Seaborn",
    "description": "Seaborn is a robust, comprehensive Python library focused on the creation of informative and attractive statistical graphics. As a data analyst, seaborn plays an essential role in elaborating complex visual stories with the data. It aids in understanding the data by providing an interface for drawing attractive and informative statistical graphics. Seaborn is built on top of Python's core visualization library Matplotlib, and is integrated with data structures from Pandas. This makes seaborn an integral tool for data visualization in the data analyst's toolkit, making the exploration and understanding of data easier and more intuitive.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Seaborn",
        "url": "https://seaborn.pydata.org/",
        "type": "article"
      },
      {
        "title": "Seaborn Tutorial : Seaborn Full Course",
        "url": "https://www.youtube.com/watch?v=6GUZXDef2U0",
        "type": "video"
      }
    ]
  },
  "-oRH7LgigHcfBkNF1xwxh": {
    "title": "Data Sources",
    "description": "Sources of data are origins or locations from which data is collected, categorized as primary (direct, firsthand information) or secondary (collected by others). Common primary sources include surveys, interviews, experiments, and sensor data. Secondary sources encompass databases, published reports, government data, books, articles, and web data like social media posts. Data sources can also be classified as internal (within an organization) or external (from outside sources).",
    "links": []
  },
  "VdMhrAi48V-JXw544YTKI": {
    "title": "Databases (SQL, No-SQL)",
    "description": "Databases are organized collections of data, stored and accessed electronically. SQL databases, like MySQL or PostgreSQL, use a structured, table-based format with a predefined schema, enforcing relationships between data through keys. NoSQL databases, such as MongoDB or Cassandra, offer more flexible data models like document, key-value, or graph, allowing for unstructured or semi-structured data and often prioritizing scalability and speed over strict consistency.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Types of Databases: Relational, NoSQL, Cloud, Vector",
        "url": "https://www.datacamp.com/blog/types-of-databases-overview",
        "type": "article"
      },
      {
        "title": "Types of Databases - MongoDB",
        "url": "https://www.mongodb.com/resources/basics/databases/types",
        "type": "article"
      }
    ]
  },
  "cxTriSZvrmXP4axKynIZW": {
    "title": "Internet",
    "description": "The internet serves as a vast repository of information that can be leveraged for machine learning projects. This includes publicly available datasets, web pages that can be scraped for relevant information, social media platforms where user-generated content provides insights into opinions and trends, and online databases that offer structured data on various topics. These sources provide a diverse range of data types, from text and images to numerical and categorical data, enabling the development of a wide array of machine learning models.",
    "links": []
  },
  "s-wUPMaagyRupT2RdfHks": {
    "title": "APIs",
    "description": "Application Programming Interfaces, better known as APIs, play a fundamental role in the work of data analysts, particularly in the process of data collection. APIs are sets of protocols, routines, and tools that enable different software applications to communicate with each other. In data analysis, APIs are used extensively to collect, exchange, and manipulate data from different sources in a secure and efficient manner. This data collection process is paramount in shaping the insights derived by the analysts.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is an API?",
        "url": "https://aws.amazon.com/what-is/api/",
        "type": "article"
      },
      {
        "title": "A Beginner's Guide to APIs",
        "url": "https://www.postman.com/what-is-an-api/",
        "type": "article"
      }
    ]
  },
  "dJZqe47kzRqYIG-4AZTlz": {
    "title": "Mobile Apps",
    "description": "Mobile app data refers to the information generated and collected from applications running on mobile devices like smartphones and tablets. This data encompasses a wide range of user interactions, device characteristics, and app performance metrics. It can include user demographics, in-app behavior, location data, device type, operating system, and network information, among other things.",
    "links": []
  },
  "KeGCHoJRHp-mBX-P5to4Y": {
    "title": "IoT",
    "description": "The Internet of Things (IoT) refers to the network of physical devices, vehicles, home appliances, and other items embedded with electronics, software, sensors, and actuators that enable these objects to connect and exchange data. These devices continuously generate vast amounts of data reflecting their status, environment, and interactions. This data can include sensor readings like temperature, pressure, humidity, location, and images or video streams.\n\n*   [@article@What is the Internet of Things (IoT)?](https://www.ibm.com/think/topics/internet-of-things)\n*   [@article@Internet of Things](https://en.wikipedia.org/wiki/Internet_of_things)\n*   [@video@What is IoT (Internet of Things)? An Introduction](https://www.youtube.com/watch?v=4FxU-xpuCww)\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is the Internet of Things (IoT)?",
        "url": "https://www.ibm.com/think/topics/internet-of-things",
        "type": "article"
      },
      {
        "title": "Internet of Things",
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "type": "article"
      },
      {
        "title": "What is the IoT",
        "url": "https://www.youtube.com/watch?v=4FxU-xpuCww",
        "type": "video"
      }
    ]
  },
  "U4LGIEE3igeE5Ed3EWzsu": {
    "title": "Data Formats",
    "description": "Data formats define the structure in which data is organized and stored. These formats dictate how information is encoded, allowing computers to interpret and process it effectively. Common examples include CSV (Comma Separated Values) for tabular data, JSON (JavaScript Object Notation) for structured data with key-value pairs, and image formats like JPEG and PNG for visual data. The choice of data format impacts storage efficiency, data accessibility, and the compatibility with different machine learning tools and algorithms.",
    "links": []
  },
  "kagKVPUyLtx8UPAFjRvbN": {
    "title": "JSON",
    "description": "JavaScript Object Notation (JSON) is a standard text-based format for representing structured data based on JavaScript object syntax. It is commonly used for transmitting data in web applications (e.g., sending some data from the server to the client, so it can be displayed on a web page, or vice versa).\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Working with JSON",
        "url": "https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON",
        "type": "article"
      },
      {
        "title": "JSON Tutorial for Beginners",
        "url": "https://www.youtube.com/watch?v=iiADhChRriM",
        "type": "video"
      }
    ]
  },
  "tq6WRwUpaCok9fX-0bY7m": {
    "title": "Parquet",
    "description": "Parquet is a columnar storage format designed for efficient data storage and retrieval. Unlike row-oriented formats, Parquet stores data by columns, which allows for better compression and faster query performance when only a subset of columns are needed. This makes it particularly well-suited for big data processing and analytics, where large datasets are common and queries often target specific columns.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Parquet",
        "url": "https://parquet.apache.org/",
        "type": "article"
      },
      {
        "title": "Parquet - Databricks",
        "url": "https://www.databricks.com/glossary/what-is-parquet",
        "type": "article"
      }
    ]
  },
  "MWfdLCb_w06A0jqwUJUxl": {
    "title": "CSV",
    "description": "CSV or Comma Separated Values files play an integral role in data collection for data analysts. These file types allow the efficient storage of data and are commonly generated by spreadsheet software like Microsoft Excel or Google Sheets, but their simplicity makes them compatible with a variety of applications that deal with data. In the context of data analysis, CSV files are extensively used to import and export large datasets, making them essential for any data analyst's toolkit. They allow analysts to organize vast amounts of information into a structured format, which is fundamental in extracting useful insights from raw data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is a CSV file: A comprehensive guide",
        "url": "https://flatfile.com/blog/what-is-a-csv-file-guide-to-uses-and-benefits/",
        "type": "article"
      },
      {
        "title": "Understanding CSV Files",
        "url": "https://www.youtube.com/watch?v=UofTplCVkYI",
        "type": "video"
      }
    ]
  },
  "K9Si7kJe946CcGWBGmDsZ": {
    "title": "Excel",
    "description": "Excel files are a common way to store data in a structured format using rows and columns. Each cell in the spreadsheet can hold different types of data, like numbers, text, or dates. These files are often used for organizing, analyzing, and visualizing data because they are easy to create and manipulate using spreadsheet software.",
    "links": []
  },
  "qRHeaD2udDaItAxmiIiUg": {
    "title": "Other Data Formats",
    "description": "Beyond the common formats like CSV, Excel, JSON, and Parquet, data can exist in a variety of other structures. These include formats optimized for specific applications or data types. For instance, images are often stored as JPEGs or PNGs, while audio data might be in WAV or MP3 format. Relational databases store data in structured tables accessible through SQL. Furthermore, specialized formats like HDF5 are used for large, complex datasets, particularly in scientific computing, and Protocol Buffers offer an efficient way to serialize structured data. Data can also be unstructured, existing as plain text, log files, or even streaming data from sensors.",
    "links": []
  },
  "MdhfkuKWTDCE73hczzG3D": {
    "title": "Preprocessing Techniques",
    "description": "Preprocessing techniques in data cleaning involve transforming raw data into a more suitable format for machine learning models. This often includes handling missing values by either imputing them with estimated values or removing rows/columns containing them. It also encompasses scaling numerical features to a similar range to prevent features with larger values from dominating the model, and encoding categorical features into numerical representations that algorithms can understand. These steps aim to improve the quality and consistency of the data, leading to better model performance.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Data Preprocessing: A Complete Guide with Python Examples",
        "url": "https://www.datacamp.com/blog/data-preprocessing",
        "type": "article"
      },
      {
        "title": "16 Data Pre Processing Techniques in 20 Minutes | Data Preprocessing in machine learning",
        "url": "https://www.youtube.com/watch?v=oggHzC_L9uc",
        "type": "video"
      }
    ]
  },
  "5v0jRBYrRuVXQC90IseRG": {
    "title": "Data Cleaning",
    "description": "Data cleaning, which is often referred as data cleansing or data scrubbing, is one of the most important and initial steps in the data analysis process. As a data analyst, the bulk of your work often revolves around understanding, cleaning, and standardizing raw data before analysis. Data cleaning involves identifying, correcting or removing any errors or inconsistencies in datasets in order to improve their quality. The process is crucial because it directly determines the accuracy of the insights you generate - garbage in, garbage out. Even the most sophisticated models and visualizations would not be of much use if they're based on dirty data. Therefore, mastering data cleaning techniques is essential for any data analyst.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Data cleaning",
        "url": "https://www.tableau.com/learn/articles/what-is-data-cleaning",
        "type": "article"
      }
    ]
  },
  "y3-nWiDjlY6ZwqmxBUvhd": {
    "title": "Dimensionality Reduction",
    "description": "Dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. Feature selection selects a subset of the original features, while feature extraction transforms the data into a lower-dimensional space. The goal is to simplify the data without losing important information, making it easier to analyze and model.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Dimensionality Reduction?",
        "url": "https://www.ibm.com/think/topics/dimensionality-reduction",
        "type": "article"
      },
      {
        "title": "Machine Learning - Dimensionality Reduction",
        "url": "https://www.youtube.com/watch?v=AU_hBML2H1c",
        "type": "video"
      }
    ]
  },
  "UmGdV94afOIbAL8MaxOWv": {
    "title": "Feature Engineering",
    "description": "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy. This involves selecting, transforming, and creating new features from existing data. The goal is to extract more information from the data and make it easier for machine learning algorithms to learn patterns and make accurate predictions.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is a feature engineering? | IBM",
        "url": "https://www.ibm.com/think/topics/feature-engineering",
        "type": "article"
      },
      {
        "title": "Feature Engineering in Machine Learning: A Practical Guide",
        "url": "https://www.datacamp.com/tutorial/feature-engineering",
        "type": "article"
      },
      {
        "title": "What is feature engineering | Feature Engineering Tutorial Python",
        "url": "https://www.youtube.com/watch?v=pYVScuY-GPk",
        "type": "video"
      }
    ]
  },
  "cigwKoltemM0q-M5O50Is": {
    "title": "Feature Selection",
    "description": "Feature selection is the process of choosing a subset of the most relevant features from your original dataset. The goal is to reduce the dimensionality of the data by removing irrelevant, redundant, or noisy features. This can lead to simpler models, improved model performance (e.g., accuracy, speed), and better understanding of the underlying data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Feature Selection? | IBM",
        "url": "https://www.ibm.com/think/topics/feature-selection",
        "type": "article"
      }
    ]
  },
  "iBkTNbk8Xz626F_a3Bo5J": {
    "title": "Feature Scaling & Normalization",
    "description": "Feature scaling is a preprocessing technique in machine learning that transforms numerical features to a common scale, ensuring they contribute equally to the model by preventing features with larger ranges from dominating, which is particularly important for algorithms sensitive to feature scales like gradient descent-based methods. Key methods include Standardization (transforming data to a mean of 0 and a standard deviation of 1, often better for outliers), and Normalization (scaling data to a fixed range, often 0 to 1).\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Normalization vs. Standardization: How to Know the Difference",
        "url": "https://www.datacamp.com/tutorial/normalization-vs-standardization",
        "type": "article"
      },
      {
        "title": "Standardization vs Normalization Clearly Explained!",
        "url": "https://www.youtube.com/watch?v=sxEqtjLC0aM",
        "type": "video"
      }
    ]
  },
  "Ns2zKn8BL_kTEI6O65pCp": {
    "title": "Types of Machine Learning",
    "description": "Machine learning algorithms learn from data to make predictions or decisions. These algorithms are broadly categorized into supervised, unsupervised, and reinforcement learning. Supervised learning uses labeled data to train a model to map inputs to outputs. Unsupervised learning, on the other hand, works with unlabeled data to discover hidden patterns and structures. Reinforcement learning involves training an agent to make decisions in an environment to maximize a reward. More recently, semi-supervised learning, which uses a combination of labeled and unlabeled data, and self-supervised learning, where the data itself provides the supervision signal, have gained prominence.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "5 types of machine learning",
        "url": "https://lumenalta.com/insights/5-types-of-machine-learning",
        "type": "article"
      }
    ]
  },
  "36ryjK5isV1MD4MgZP2Jn": {
    "title": "Unsupervised Learning",
    "description": "Unsupervised learning is a type of machine learning where the algorithm learns patterns from unlabeled data. Unlike supervised learning, there are no pre-defined correct answers or target variables provided to guide the learning process. Instead, the algorithm explores the data to discover hidden structures, relationships, and groupings on its own. Common tasks in unsupervised learning include clustering, dimensionality reduction, and anomaly detection.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Unsupervised Learning?",
        "url": "https://cloud.google.com/discover/what-is-unsupervised-learning",
        "type": "article"
      },
      {
        "title": "Introduction to Unsupervised Learning",
        "url": "https://www.datacamp.com/blog/introduction-to-unsupervised-learning",
        "type": "article"
      },
      {
        "title": "Unsupervised Machine Learning Explained For Beginners",
        "url": "https://www.youtube.com/watch?v=yteYU_QpUxs",
        "type": "video"
      }
    ]
  },
  "Yho0zf9F-ROhEnTxRMq_M": {
    "title": "Semi-supervised Learning",
    "description": "Semi-supervised learning is a type of machine learning where the training data contains both labeled and unlabeled examples. The goal is to leverage the information from the unlabeled data to improve the performance of a model that would otherwise be trained solely on the labeled data. This approach is particularly useful when obtaining labels is expensive or time-consuming, but unlabeled data is readily available.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Semi-Supervised Learning?",
        "url": "https://www.ibm.com/think/topics/semi-supervised-learning",
        "type": "article"
      },
      {
        "title": "What is Semi-Supervised Learning?",
        "url": "https://www.youtube.com/watch?v=C3Lr6Waw66g",
        "type": "video"
      }
    ]
  },
  "5MUwKGfSTKlam8KCG0A1U": {
    "title": "Supervised Learning",
    "description": "Supervised learning is a type of machine learning where an algorithm learns from a labeled dataset. This means that each data point in the dataset is paired with a corresponding correct output, or \"label.\" The algorithm's goal is to learn a function that maps inputs to outputs, so that when given new, unseen inputs, it can predict the correct output based on the patterns it learned from the labeled data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Supervised Learning?",
        "url": "https://cloud.google.com/discover/what-is-supervised-learning",
        "type": "article"
      },
      {
        "title": "Supervised Machine Learning",
        "url": "https://www.datacamp.com/blog/supervised-machine-learning",
        "type": "article"
      },
      {
        "title": "Supervised Machine Learning Explained For Beginners",
        "url": "https://www.youtube.com/watch?v=Mu3POlNoLdc",
        "type": "video"
      }
    ]
  },
  "NC1A2SQVyc1n-KEf6yl-4": {
    "title": "Reinforcement Learning",
    "description": "Reinforcement learning is a type of machine learning where an agent learns to make decisions in an environment to maximize a reward. The agent interacts with the environment, takes actions, and receives feedback in the form of rewards or penalties. Through trial and error, the agent learns a policy that maps states to actions, aiming to accumulate the most reward over time.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is reinforcement learning?",
        "url": "https://online.york.ac.uk/resources/what-is-reinforcement-learning/",
        "type": "article"
      },
      {
        "title": "Resources to Learn Reinforcement Learning",
        "url": "https://towardsdatascience.com/best-free-courses-and-resources-to-learn-reinforcement-learning-ed6633608cb2/",
        "type": "article"
      },
      {
        "title": "https://huggingface.co/learn/deep-rl-course/unit0/introduction",
        "url": "https://huggingface.co/learn/deep-rl-course/unit0/introduction",
        "type": "article"
      },
      {
        "title": "Reinforcement Learning in 3 Hours | Full Course using Python",
        "url": "https://www.youtube.com/watch?v=Mut_u40Sqz4",
        "type": "article"
      }
    ]
  },
  "lgO7luG7-R_FY5nwFjRE0": {
    "title": "Self-supervised Learning",
    "description": "Self-supervised learning is a type of machine learning where the model learns from unlabeled data by creating its own supervisory signals. This is achieved by masking parts of the input data and training the model to predict the masked portions based on the remaining data. In essence, the data itself provides the labels, allowing the model to learn useful representations without requiring explicit human annotation.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What Is Self-Supervised Learning?",
        "url": "https://www.ibm.com/think/topics/self-supervised-learning",
        "type": "article"
      },
      {
        "title": "What is Self Supervised Learning?",
        "url": "https://www.youtube.com/watch?v=sJzuNAisXHA",
        "type": "video"
      },
      {
        "title": "Mark Zuckerberg: AI Learns More Efficiently With Self-Supervised Learning",
        "url": "https://www.youtube.com/watch?v=R8DduzhT3-w",
        "type": "video"
      }
    ]
  },
  "rzhVFzl5H5MWtcvr8ayRk": {
    "title": "What is Machine Learning?",
    "description": "Machine learning is a field of computer science that focuses on enabling computers to learn from data without being explicitly programmed. Instead of relying on pre-defined rules, machine learning algorithms identify patterns, make predictions, and improve their performance over time as they are exposed to more data. This learning process allows machines to adapt to new situations and solve complex problems that are difficult or impossible to address with traditional programming techniques.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Machine Learning: The Basics",
        "url": "https://alexjungaalto.github.io/MLBasicsBook.pdf",
        "type": "article"
      },
      {
        "title": "What is Machine Learning (ML)?",
        "url": "https://www.ibm.com/topics/machine-learning",
        "type": "article"
      },
      {
        "title": "What is Machine Learning?",
        "url": "https://www.youtube.com/watch?v=9gGnTQTYNaE",
        "type": "video"
      },
      {
        "title": "Complete Machine Learning in One Video | Machine Learning Tutorial For Beginners 2025 | Simplilearn",
        "url": "https://www.youtube.com/watch?v=PtYRUoJRE9s",
        "type": "video"
      }
    ]
  },
  "ajKU5CPlbn7BbWHEhUNaB": {
    "title": "What is Supervised Learning?",
    "description": "Supervised learning is a type of machine learning where an algorithm learns from a labeled dataset. This means that each data point in the dataset is paired with a corresponding correct output, or \"label.\" The algorithm's goal is to learn a function that maps inputs to outputs, so that when given new, unseen inputs, it can predict the correct output based on the patterns it learned from the labeled data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Supervised Learning?",
        "url": "https://cloud.google.com/discover/what-is-supervised-learning",
        "type": "article"
      },
      {
        "title": "Supervised Machine Learning",
        "url": "https://www.datacamp.com/blog/supervised-machine-learning",
        "type": "article"
      },
      {
        "title": "Supervised Machine Learning Explained For Beginners",
        "url": "https://www.youtube.com/watch?v=Mu3POlNoLdc",
        "type": "video"
      }
    ]
  },
  "cffITx6oAcnvJlK1VLdi8": {
    "title": "Classification",
    "description": "Classification is a type of supervised learning where the goal is to assign data points to predefined categories or classes. Given a set of labeled data (where each data point has a known class), the algorithm learns a mapping function that can predict the class label for new, unseen data. The output is a discrete value representing the predicted class.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Classification - Google Crash Course",
        "url": "https://developers.google.com/machine-learning/crash-course/classification",
        "type": "course"
      },
      {
        "title": "What is Classification in Machine Learning?",
        "url": "https://www.ibm.com/think/topics/classification-machine-learning",
        "type": "article"
      },
      {
        "title": "Classification in Machine Learning: A Guide for Beginners",
        "url": "https://www.datacamp.com/blog/classification-machine-learning",
        "type": "article"
      }
    ]
  },
  "aHOjajXwkDMOssqW1VGrm": {
    "title": " Logistic Regression ",
    "description": "Logistic Regression is a method used to predict the probability of a categorical outcome. Instead of predicting a continuous value, it predicts whether something belongs to a certain category (like yes/no, true/false, or 0/1). It does this by using a logistic function (also known as a sigmoid function) to squeeze the output of a linear equation between 0 and 1, representing the probability of belonging to that category. The model learns the best coefficients for the linear equation based on the training data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Logistic Regression | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html",
        "type": "article"
      },
      {
        "title": "Understanding Logistic Regression in Python",
        "url": "https://www.datacamp.com/tutorial/understanding-logistic-regression-python",
        "type": "article"
      },
      {
        "title": "Hands-On Machine Learning: Logistic Regression with Python and Scikit-Learn",
        "url": "https://m.youtube.com/watch?v=aL21Y-u0SRs&pp=0gcJCfwAo7VqN5tD",
        "type": "video"
      }
    ]
  },
  "_jS66rGAWecXH3zVF-5ds": {
    "title": "Support Vector Machines",
    "description": "Support Vector Machines (SVMs) are a type of supervised learning algorithm used for classification and regression. They work by finding an optimal hyperplane that separates data points belonging to different classes with the largest possible margin. This hyperplane acts as a decision boundary, and new data points are classified based on which side of the hyperplane they fall on.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Support Vector Classification (SVC) - scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html",
        "type": "article"
      },
      {
        "title": "Support Vector Machines with Scikit-learn Tutorial",
        "url": "https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python",
        "type": "article"
      },
      {
        "title": "Mastering Support Vector Machines with Python and Scikit-Learn",
        "url": "https://www.youtube.com/watch?v=kPkwf1x7zpU",
        "type": "video"
      }
    ]
  },
  "x7vlCNAxfJzobj9HcTaJy": {
    "title": "K-Nearest Neighbors (KNN)",
    "description": "K-Nearest Neighbors (KNN) is a simple algorithm used for classifying data points based on their proximity to other data points. Given a new, unclassified data point, KNN identifies the 'K' closest data points (neighbors) from the training dataset. The class that appears most frequently among these 'K' neighbors is then assigned as the class of the new data point.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Nearest Neighbors | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/neighbors.html",
        "type": "article"
      },
      {
        "title": "K-Nearest Neighbors (KNN) Classification with scikit-learn",
        "url": "https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn",
        "type": "article"
      },
      {
        "title": "How to Build Your First KNN Python Model in scikit-learn (K Nearest Neighbors)",
        "url": "https://www.youtube.com/watch?v=Nz73vXn5afE",
        "type": "video"
      }
    ]
  },
  "JuTTbL_pm1ltGvhUsIzQd": {
    "title": "Gradient Boosting Machines",
    "description": "Gradient Boosting Machines (GBMs) are a type of ensemble learning algorithm that combines the predictions from multiple weaker models, typically decision trees, to create a stronger, more accurate model. The algorithm works iteratively, with each new tree trained to correct the errors made by the previous trees. This correction is achieved by focusing on the instances that were poorly predicted by the existing ensemble, effectively \"boosting\" the performance on those difficult cases. The final prediction is made by aggregating the predictions of all the trees in the ensemble.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Gradient Boosting Classifier | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html",
        "type": "article"
      },
      {
        "title": "A Guide to The Gradient Boosting Algorithm",
        "url": "https://www.datacamp.com/tutorial/guide-to-the-gradient-boosting-algorithm",
        "type": "article"
      },
      {
        "title": "Gradient Boosting in Scikit-Learn: Hands-On Tutorial",
        "url": "https://www.youtube.com/watch?v=E2mCaIZNE2g",
        "type": "video"
      }
    ]
  },
  "arlmRF5pYglsbHb-HR-2x": {
    "title": "Decision Trees, Random Forest",
    "description": "Decision Trees are a way to make predictions by learning decision rules from data features. Imagine a flowchart where each internal node represents a test on an attribute (like \"Is the color red?\"), each branch represents the outcome of the test, and each leaf node represents a class label (like \"apple\" or \"banana\"). Random Forests improve upon this by creating multiple decision trees on different subsets of the data and features, then combining their predictions to get a more accurate and robust result.",
    "links": []
  },
  "gKu6tnpTO2PhDDMYp2u7F": {
    "title": "Regression",
    "description": "Regression in supervised learning is a method used to predict a continuous numerical value. It works by finding the relationship between input features (independent variables) and a target variable (dependent variable). The goal is to build a model that can accurately estimate the target variable's value based on the given input features. For example, predicting house prices based on size and location is a regression problem.",
    "links": []
  },
  "xGO1X9aZRgKcgzi6r1xq8": {
    "title": "Linear Regression",
    "description": "Linear regression is a simple method used to find the best straight line that describes the relationship between a dependent variable (the one you're trying to predict) and one or more independent variables (the ones you're using to make the prediction). It works by finding the line that minimizes the sum of the squared differences between the actual values and the values predicted by the line. This line can then be used to predict the dependent variable for new values of the independent variables.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Linear Regression | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/linear_model.html",
        "type": "article"
      },
      {
        "title": "Sklearn Linear Regression: A Complete Guide with Examples",
        "url": "https://www.datacamp.com/tutorial/sklearn-linear-regression",
        "type": "article"
      },
      {
        "title": "Hands-On Linear Regression with Scikit-Learn in Python",
        "url": "https://www.youtube.com/watch?v=ukZn2RJb7TU",
        "type": "video"
      }
    ]
  },
  "hfr2MU8QkVt9KhVi1Okpv": {
    "title": "Polynomial Regression",
    "description": "Polynomial regression is a type of supervised learning algorithm used when the relationship between the input features and the output variable is non-linear. Instead of fitting a straight line, it fits a polynomial equation to the data. This allows the model to capture curves and more complex relationships, potentially leading to better predictions when a linear model is insufficient. The degree of the polynomial determines the complexity of the curve that can be fitted.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Polynomial Features | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html",
        "type": "article"
      },
      {
        "title": "Polynomial Regression in Python using scikit-learn (with a practical example)",
        "url": "https://data36.com/polynomial-regression-python-scikit-learn/",
        "type": "article"
      },
      {
        "title": "Build a Polynomial Regression Model in Python using Scikit-Learn",
        "url": "https://medium.com/@renadalhendy/build-a-polynomial-regression-model-in-python-using-scikit-learn-1b5fd31beb02",
        "type": "article"
      }
    ]
  },
  "9oWdnQd-vwVJi62JQLgJ5": {
    "title": "What is Unsupervised Learning?",
    "description": "Unsupervised learning is a type of machine learning where the algorithm learns patterns from unlabeled data. Unlike supervised learning, there are no pre-defined correct answers or target variables provided to guide the learning process. Instead, the algorithm explores the data to discover hidden structures, relationships, and groupings on its own. Common tasks in unsupervised learning include clustering, dimensionality reduction, and anomaly detection.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Unsupervised Learning?",
        "url": "https://cloud.google.com/discover/what-is-unsupervised-learning",
        "type": "article"
      },
      {
        "title": "Introduction to Unsupervised Learning",
        "url": "https://www.datacamp.com/blog/introduction-to-unsupervised-learning",
        "type": "article"
      },
      {
        "title": "Unsupervised Machine Learning Explained For Beginners",
        "url": "https://www.youtube.com/watch?v=yteYU_QpUxs",
        "type": "video"
      }
    ]
  },
  "CBSGvGPoI53p7BezXNm6M": {
    "title": "Clustering",
    "description": "Clustering is a way to automatically group similar data points together. Imagine you have a bunch of scattered objects and you want to organize them into piles based on how alike they are. Clustering algorithms do this by finding patterns in the data, grouping data points that are close to each other in some way, and separating them from data points that are further apart. The algorithm decides which data points belong to which group, without you telling it what the groups should be beforehand.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Clustering | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/clustering.html",
        "type": "article"
      },
      {
        "title": "What is clustering?",
        "url": "https://developers.google.com/machine-learning/clustering/overview",
        "type": "article"
      }
    ]
  },
  "vQI-4uFQJ6694nm1SCpDR": {
    "title": "Dimensionality Reduction",
    "description": "Dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. Feature selection selects a subset of the original features, while feature extraction transforms the data into a lower-dimensional space. The goal is to simplify the data without losing important information, making it easier to analyze and model.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Dimensionality Reduction?",
        "url": "https://www.ibm.com/think/topics/dimensionality-reduction",
        "type": "article"
      },
      {
        "title": "Machine Learning - Dimensionality Reduction",
        "url": "https://www.youtube.com/watch?v=AU_hBML2H1c",
        "type": "video"
      }
    ]
  },
  "BzUunjJrUMlh6K1NOOD87": {
    "title": "Overlapping",
    "description": "Overlapping clustering allows data points to belong to multiple clusters simultaneously. Unlike traditional \"hard\" clustering where each point is assigned to only one cluster, overlapping clustering acknowledges that data points can exhibit characteristics of several groups. This is particularly useful when dealing with complex datasets where boundaries between clusters are not well-defined. One algorithm that implements overlapping clustering is the _Fuzzy C-Means (FCM)_ algorithm. FCM assigns a membership degree to each data point for each cluster, representing the probability of belonging to that cluster. A data point can have non-zero membership degrees for multiple clusters, indicating its partial membership in each.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Unsupervised Clustering: A Guide",
        "url": "https://builtin.com/articles/unsupervised-clustering",
        "type": "article"
      }
    ]
  },
  "jJ8cXfHV2LG5PJGZRTHxB": {
    "title": "Hierarchical",
    "description": "Hierarchical clustering is a method of grouping data points into clusters based on their similarity, building a hierarchy of clusters. It starts by treating each data point as its own cluster and then iteratively merges the closest clusters until only one cluster remains, or a stopping criterion is met. This process creates a tree-like structure called a dendrogram, which visually represents the hierarchy of clusters. Scikit-learn provides an implementation of agglomerative hierarchical clustering through its `AgglomerativeClustering` class, which allows you to specify the linkage criterion (e.g., ward, complete, average) to determine how the distance between clusters is calculated.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Hierarchical clustering | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering",
        "type": "article"
      },
      {
        "title": "What is Hierarchical Clustering?",
        "url": "https://www.ibm.com/think/topics/hierarchical-clustering",
        "type": "article"
      }
    ]
  },
  "eErzKbR8sRNlrYcwNSRSh": {
    "title": "Exclusive",
    "description": "Exclusive clustering, also known as hard clustering, is a type of clustering where each data point can only belong to one cluster. This means there's no overlap between clusters; a data point is definitively assigned to a single group. The goal is to partition the data into distinct, non-overlapping clusters based on similarity. For example, K-Means is an exclusive clustering algorithm. It aims to partition n data points into k clusters in which each data point belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "K-means | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/clustering.html#k-means",
        "type": "article"
      },
      {
        "title": "Unsupervised Clustering: A Guide",
        "url": "https://builtin.com/articles/unsupervised-clustering",
        "type": "article"
      }
    ]
  },
  "VrLaUipVKWvwnFF0ZbIlo": {
    "title": "Probabilistic",
    "description": "Probabilistic clustering assumes that the data is generated from a mixture of probability distributions. Instead of assigning each data point to a single cluster, it provides the probability of a data point belonging to each cluster. A common example is the Gaussian Mixture Model (GMM), where it's assumed that the data points are generated from a mixture of Gaussian distributions. Scikit-learn provides an implementation of GMM that can be used for probabilistic clustering.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Gaussian mixture models",
        "url": "https://scikit-learn.org/stable/modules/mixture.html#mixture",
        "type": "article"
      },
      {
        "title": "Gaussian Mixture Model Explained",
        "url": "https://scikit-learn.org/stable/modules/mixture.html#mixture",
        "type": "article"
      }
    ]
  },
  "owSUO9Ut9sggd1OiWr3O7": {
    "title": "Autoencoders",
    "description": "Autoencoders are a type of neural network used to learn efficient data representations in an unsupervised manner. They work by compressing the input data into a lower-dimensional \"code\" and then reconstructing the original input from this compressed representation. By forcing the network to learn a compressed version of the data, autoencoders can discover important features and reduce the dimensionality of the data, making it easier to process and analyze.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What Is an Autoencoder? | IBM",
        "url": "https://www.ibm.com/think/topics/autoencoder",
        "type": "article"
      }
    ]
  },
  "K-x_L3z8JTSHwtTeHm4EG": {
    "title": "Principal Component Analysis ",
    "description": "Principal Component Analysis (PCA) is a technique used to reduce the number of variables in a dataset while preserving the most important information. It transforms the original variables into a new set of variables called principal components, which are ordered by the amount of variance they explain. The first principal component captures the most variance, the second captures the second most, and so on. By selecting a smaller number of these principal components, you can reduce the dimensionality of the data without losing too much information.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "PCA | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html",
        "type": "article"
      },
      {
        "title": "What is principal component analysis (PCA)?",
        "url": "https://www.ibm.com/think/topics/principal-component-analysis",
        "type": "article"
      },
      {
        "title": "PCA Analysis in Python Explained (Scikit - Learn)",
        "url": "https://www.youtube.com/watch?v=6uwa9EkUqpg",
        "type": "video"
      }
    ]
  },
  "EtU_9MOklVBvnvyg30Yfx": {
    "title": "What is Reinforcement Learning?",
    "description": "Reinforcement learning is a type of machine learning where an agent learns to make decisions in an environment to maximize a reward. The agent interacts with the environment, takes actions, and receives feedback in the form of rewards or penalties. Through trial and error, the agent learns a policy that maps states to actions, aiming to accumulate the most reward over time.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is reinforcement learning?",
        "url": "https://online.york.ac.uk/resources/what-is-reinforcement-learning/",
        "type": "article"
      },
      {
        "title": "Resources to Learn Reinforcement Learning",
        "url": "https://towardsdatascience.com/best-free-courses-and-resources-to-learn-reinforcement-learning-ed6633608cb2/",
        "type": "article"
      },
      {
        "title": "https://huggingface.co/learn/deep-rl-course/unit0/introduction",
        "url": "https://huggingface.co/learn/deep-rl-course/unit0/introduction",
        "type": "article"
      },
      {
        "title": "Reinforcement Learning in 3 Hours | Full Course using Python",
        "url": "https://www.youtube.com/watch?v=Mut_u40Sqz4",
        "type": "video"
      }
    ]
  },
  "PZ-WxKGTcWTrXmYI_inD_": {
    "title": "Policy Gradient",
    "description": "Policy Gradient methods are a type of reinforcement learning algorithm that directly optimizes the policy function, which maps states to actions. Instead of learning a value function (like Q-learning) and then deriving a policy from it, policy gradient methods directly learn the optimal policy by adjusting its parameters to maximize the expected reward. This is typically done by estimating the gradient of the expected reward with respect to the policy parameters and then updating the parameters in the direction of the gradient.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Policy Gradient Theorem Explained: A Hands-On Introduction",
        "url": "https://www.datacamp.com/tutorial/policy-gradient-theorem",
        "type": "article"
      },
      {
        "title": "An introduction to Policy Gradient methods - Deep Reinforcement Learning",
        "url": "https://www.youtube.com/watch?v=5P7I-xPq8u8",
        "type": "video"
      }
    ]
  },
  "4Vy6lW9vF_SWwbKLU0qno": {
    "title": "Actor-Critic Methods",
    "description": "Actor-Critic methods in reinforcement learning are a type of algorithm that combines the strengths of both value-based and policy-based approaches. They use two separate models: an \"actor\" that learns the optimal policy (how to act), and a \"critic\" that estimates the value function (how good a state or action is). The critic evaluates the actor's actions, providing feedback that helps the actor improve its policy, while the actor uses this feedback to refine its decision-making process.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Actor-critic algorithm",
        "url": "https://en.wikipedia.org/wiki/Actor-critic_algorithm",
        "type": "article"
      },
      {
        "title": "Everything You Need To Master Actor Critic Methods | Tensorflow 2 Tutorial",
        "url": "https://www.youtube.com/watch?v=LawaN3BdI00",
        "type": "article"
      }
    ]
  },
  "9o-ZT9oZIE3hCXD6eWZI0": {
    "title": "Deep-Q Networks",
    "description": "Deep Q-Networks (DQNs) are a type of reinforcement learning algorithm that combines Q-learning with deep neural networks. Instead of using a traditional Q-table to store Q-values (which represent the expected reward for taking a specific action in a specific state), DQNs use a neural network to approximate the Q-function. This allows DQNs to handle environments with large or continuous state spaces where a Q-table would be impractical. The neural network takes the state as input and outputs the Q-values for each possible action, enabling the agent to learn optimal policies through trial and error.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "The Deep Q-Learning Algorithm",
        "url": "https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm",
        "type": "article"
      },
      {
        "title": "Deep Q-Learning/Deep Q-Network (DQN) Explained | Python Pytorch Deep Reinforcement Learning",
        "url": "https://www.youtube.com/watch?v=EUrWGTCGzlA",
        "type": "video"
      }
    ]
  },
  "wxq5dkrpgvs3axmLmeHCk": {
    "title": "Q-Learning",
    "description": "Q-Learning is a type of reinforcement learning algorithm that aims to find the best action to take given the current state. It works by learning a \"Q-function,\" which estimates the expected cumulative reward for taking a specific action in a particular state and following the optimal policy thereafter. This Q-function is iteratively updated based on the agent's experiences, allowing it to learn the optimal policy without needing a model of the environment.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "An Introduction to Q-Learning: A Tutorial For Beginners",
        "url": "https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial",
        "type": "article"
      },
      {
        "title": "A Gentle Introduction to Q-Learning",
        "url": "https://machinelearningmastery.com/a-gentle-introduction-to-q-learning/",
        "type": "article"
      },
      {
        "title": "What is Q-Learning (back to basics)",
        "url": "https://www.youtube.com/watch?v=nOBm4aYEYR4",
        "type": "video"
      }
    ]
  },
  "99TI95HVGrXIYr-PIDxhC": {
    "title": "What is Model Evaluation?",
    "description": "Model evaluation is the process of assessing how well a machine learning model performs on a given dataset. It involves using various metrics and techniques to quantify the model's accuracy, reliability, and generalization ability. This helps determine if the model is suitable for deployment and whether further improvements are needed.",
    "links": []
  },
  "5dKl6SUQhOsZfUtVR5hzw": {
    "title": "Metrics to Evaluate",
    "description": "Model evaluation metrics are quantitative measures used to assess the performance of a machine learning model. These metrics provide insights into how well the model is generalizing to unseen data and help in comparing different models or tuning hyperparameters. They quantify various aspects of model behavior, such as accuracy, precision, recall, and error rate, allowing data scientists to make informed decisions about model selection and deployment.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Metrics and scoring: quantifying the quality of predictions | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/model_evaluation.html",
        "type": "article"
      }
    ]
  },
  "3wib9UH0_OLhKjqKoZEMv": {
    "title": "Accuracy",
    "description": "Accuracy measures how often a machine learning model correctly predicts the outcome. It's calculated by dividing the number of correct predictions by the total number of predictions made. The formula for accuracy is: (Number of Correct Predictions) / (Total Number of Predictions).\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Accuracy | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html",
        "type": "article"
      },
      {
        "title": "Machine Learning Model Accuracy",
        "url": "https://www.giskard.ai/glossary/machine-learning-model-accuracy",
        "type": "article"
      }
    ]
  },
  "mja35tndhAT5z_ysv-hDe": {
    "title": "Precision",
    "description": "Precision measures how accurate a model's positive predictions are. It tells you, out of all the instances the model predicted as positive, what proportion were actually positive. In simpler terms, it answers the question: \"When the model says something is true, how often is it actually true?\". The formula for precision is: Precision = True Positives / (True Positives + False Positives).\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Precision | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html",
        "type": "article"
      }
    ]
  },
  "DH33Na9zz_WGmbD-Dxvq1": {
    "title": "Recall",
    "description": "Recall measures how well a model identifies all the actual positive cases. It answers the question: \"Of all the actual positive instances, how many did the model correctly predict as positive?\". A high recall means the model is good at minimizing false negatives. The formula for recall is: `Recall = True Positives / (True Positives + False Negatives)`.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Recall | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html",
        "type": "article"
      }
    ]
  },
  "FdNY8QUbPPpeHFgD8TTaD": {
    "title": "F1-Score",
    "description": "The F1-score is a way to measure how accurate a model is, considering both precision and recall. Precision tells you how many of the positive predictions made by the model were actually correct. Recall tells you how many of the actual positive cases the model was able to identify. The F1-score balances these two metrics, giving a single score that represents the overall performance of the model. It's calculated as the harmonic mean of precision and recall:`F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "F1-score | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html",
        "type": "article"
      }
    ]
  },
  "anEGWHVpcp75e3jQrj_LZ": {
    "title": "ROC-AUC",
    "description": "ROC-AUC, or Receiver Operating Characteristic Area Under the Curve, is a performance measurement for classification problems at various threshold settings. ROC is a probability curve that plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at different threshold values. AUC measures the entire two-dimensional area underneath the entire ROC curve from (0,0) to (1,1). AUC provides an aggregate measure of performance across all possible classification thresholds.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "ROC-AUC score | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html",
        "type": "article"
      },
      {
        "title": "AUC and the ROC Curve in Machine Learning",
        "url": "https://www.datacamp.com/tutorial/auc",
        "type": "article"
      },
      {
        "title": "Classification: ROC and AUC",
        "url": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc",
        "type": "article"
      }
    ]
  },
  "7fOp3t283GeOn6Tf4kEuN": {
    "title": "Log Loss",
    "description": "Log Loss, also known as cross-entropy loss, quantifies the performance of a classification model where the prediction input is a probability value between 0 and 1. It measures the uncertainty of the model's predicted probabilities compared to the actual labels. Lower Log Loss values indicate better calibrated predictions, meaning the predicted probabilities align more closely with the true outcomes.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "log_loss | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html",
        "type": "article"
      },
      {
        "title": "Intuition behind Log-loss Score",
        "url": "https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a/",
        "type": "article"
      }
    ]
  },
  "oyL0M2OP4NTNbIO3zq-Hz": {
    "title": "Confusion Matrix",
    "description": "A confusion matrix is a table that summarizes the performance of a classification model. It displays the counts of true positive, true negative, false positive, and false negative predictions, allowing for a detailed analysis of the model's accuracy and types of errors it makes. This breakdown helps in understanding where the model excels and where it struggles, providing insights beyond simple accuracy scores.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Confusion matrix | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html",
        "type": "article"
      },
      {
        "title": "What is A Confusion Matrix in Machine Learning? The Model Evaluation Tool Explained",
        "url": "https://www.datacamp.com/tutorial/what-is-a-confusion-matrix-in-machine-learning",
        "type": "article"
      }
    ]
  },
  "LWLqa61GK5ukYzHpjinYi": {
    "title": "Forward propagation",
    "description": "Forward propagation is the process of feeding input data through a neural network to generate an output. It involves taking the inputs, multiplying them by weights, adding biases, and then passing the result through an activation function at each layer of the network. This process is repeated layer by layer until the final output is produced.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Forward Propagation in Neural Networks: A Complete Guide",
        "url": "https://www.datacamp.com/tutorial/forward-propagation-neural-networks",
        "type": "article"
      },
      {
        "title": "Forward Propagation in Neural Networks",
        "url": "https://www.youtube.com/watch?v=99CcviQchd8",
        "type": "video"
      }
    ]
  },
  "0meihv22e11GwqnRdSJ9g": {
    "title": "Back Propagation",
    "description": "",
    "links": []
  },
  "8425N_E43Dv5mcmEcXRIa": {
    "title": "Perceptron, Multi-layer Perceptrons",
    "description": "A perceptron is a fundamental building block of neural networks, acting as a single-layer linear classifier. It takes several inputs, multiplies each by a weight, sums them up, and then applies an activation function to produce an output. Multi-layer perceptrons (MLPs) extend this concept by stacking multiple layers of perceptrons, including an input layer, one or more hidden layers, and an output layer, allowing for the modeling of more complex, non-linear relationships in data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Perceptron",
        "url": "https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron",
        "type": "article"
      },
      {
        "title": "The Perceptron Explained",
        "url": "https://www.youtube.com/watch?v=i1G7PXZMnSc",
        "type": "video"
      }
    ]
  },
  "RXTci1N6i6D9HqTbsLYIy": {
    "title": "Activation Functions",
    "description": "Activation functions in neural networks determine the output of a node given an input or set of inputs. They introduce non-linearity into the network, allowing it to learn complex patterns and relationships in data. Without activation functions, a neural network would simply be a linear regression model, severely limiting its ability to model intricate data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Activation Functions in Neural Networks: How to Choose the Right One",
        "url": "https://towardsdatascience.com/activation-functions-in-neural-networks-how-to-choose-the-right-one-cb20414c04e5/",
        "type": "article"
      },
      {
        "title": "Neural networks: Activation functions",
        "url": "https://developers.google.com/machine-learning/crash-course/neural-networks/activation-functions",
        "type": "article"
      },
      {
        "title": "Activation Functions In Neural Networks Explained",
        "url": "https://www.youtube.com/watch?v=Fu273ovPBmQ",
        "type": "video"
      }
    ]
  },
  "4dxZmLg0UEaaVEORupOOC": {
    "title": "Neural Network (NN) Basics",
    "description": "A neural network is a computational model inspired by the structure and function of biological neural networks. It consists of interconnected nodes, called neurons, organized in layers. These neurons process and transmit signals, learning complex patterns from data through adjusting the strengths of the connections (weights) between them.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Practical Deep Learning",
        "url": "https://course.fast.ai/",
        "type": "course"
      },
      {
        "title": "Neural Networks Explained in 5 minutes",
        "url": "https://www.youtube.com/watch?v=jmmW0F0biz0",
        "type": "video"
      }
    ]
  },
  "KcfFjpxFTFxI6HR6hBPrl": {
    "title": "Loss Functions",
    "description": "Loss functions measure how well the network's predictions match the actual values. They quantify the difference between the predicted output and the true output for a given input. The goal during training is to minimize this loss, guiding the network to adjust its internal parameters (weights and biases) to make more accurate predictions. Different loss functions are suitable for different types of problems, such as regression or classification.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Loss Functions and Their Use In Neural Networks",
        "url": "https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9/",
        "type": "article"
      },
      {
        "title": "What is Loss Function? | IBM",
        "url": "https://www.ibm.com/think/topics/loss-function",
        "type": "article"
      },
      {
        "title": "Loss in a Neural Network explained",
        "url": "https://www.youtube.com/watch?v=Skc8nqJirJg",
        "type": "video"
      }
    ]
  },
  "ZVbnAF9I1r8qWFFYG6nXv": {
    "title": "Scikit-learn",
    "description": "Scikit-learn is a free and open-source Python library that provides simple and efficient tools for data analysis and machine learning. It features various algorithms for classification, regression, clustering, dimensionality reduction, model selection, and preprocessing. It's built on NumPy, SciPy, and matplotlib, making it easy to integrate with other scientific Python libraries.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "scikit-learn",
        "url": "https://github.com/scikit-learn/scikit-learn",
        "type": "opensource"
      },
      {
        "title": "scikit-learn: machine learning in Python",
        "url": "https://scikit-learn.org/",
        "type": "article"
      },
      {
        "title": "Scikit-learn Crash Course - Machine Learning Library for Python",
        "url": "https://www.youtube.com/watch?v=0B5eIE_1vpU",
        "type": "video"
      }
    ]
  },
  "1aX_vO5zxfTV8_kUIFHkR": {
    "title": "Ridge",
    "description": "Ridge Regression is a linear regression technique that adds a penalty to the size of the coefficients. This penalty, known as L2 regularization, shrinks the coefficient values towards zero. By adding this constraint, Ridge Regression aims to reduce the model's complexity and prevent overfitting, especially when dealing with datasets that have high multicollinearity (where predictor variables are highly correlated).\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Ridge | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html",
        "type": "article"
      },
      {
        "title": "Mastering Ridge Regression in Python with scikit-learnv",
        "url": "https://www.youtube.com/watch?v=GMF4Td7KtB0",
        "type": "video"
      }
    ]
  },
  "EXogp25SPW1bBfb1gRDAe": {
    "title": "Lasso",
    "description": "Lasso (Least Absolute Shrinkage and Selection Operator) regression is a linear regression technique that adds a penalty term to the ordinary least squares (OLS) objective function. This penalty is based on the absolute values of the coefficients, effectively shrinking some coefficients towards zero. This shrinkage not only helps prevent overfitting, especially when dealing with high-dimensional data, but also performs feature selection by potentially eliminating less important features from the model.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Lasso | scikit-learn",
        "url": "https://www.ibm.com/think/topics/lasso-regression",
        "type": "article"
      },
      {
        "title": "What is lasso regression?",
        "url": "https://www.ibm.com/think/topics/lasso-regression",
        "type": "article"
      },
      {
        "title": "Lasso Regression with Scikit-Learn (Beginner Friendly)",
        "url": "https://www.youtube.com/watch?v=LmpBt0tenJE",
        "type": "video"
      }
    ]
  },
  "0hi0LdCtj9Paimgfc-l1O": {
    "title": "Validation Techniques",
    "description": "Validation techniques are methods used to estimate how well a machine learning model will generalize to unseen data. They involve splitting the available data into different subsets: a training set used to train the model, and a validation set used to evaluate the model's performance during training or hyperparameter tuning. By assessing the model on data it hasn't seen before, validation techniques help to identify issues like overfitting and underfitting, and ultimately guide the selection of the best model for a given task.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Cross-validation: evaluating estimator performance | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/cross_validation.html",
        "type": "article"
      },
      {
        "title": "The 5 Stages of Machine Learning Validation",
        "url": "https://towardsdatascience.com/the-5-stages-of-machine-learning-validation-162193f8e5db/",
        "type": "article"
      },
      {
        "title": "What is the Difference Between Test and Validation Datasets?",
        "url": "https://machinelearningmastery.com/difference-test-validation-datasets/",
        "type": "article"
      },
      {
        "title": "Validating Machine Learning Model and Avoiding Common Challenges",
        "url": "https://www.youtube.com/watch?v=TnIh2b2Rw6",
        "type": "video"
      }
    ]
  },
  "GKMhIXEuSKdW75-24Zopb": {
    "title": "LOOCV",
    "description": "Leave-One-Out Cross-Validation (LOOCV) is a specific type of cross-validation where each single data point in the dataset is used as the test set, while the remaining data points form the training set. This process is repeated for every data point, resulting in as many models being trained and evaluated as there are data points in the original dataset. The final performance metric is then calculated by averaging the performance across all these individual evaluations.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "LOOCV for Evaluating Machine Learning Algorithms",
        "url": "https://www.google.com/search?q=LOOCV&rlz=1C5GCEM_enES1173ES1173&oq=LOOCV&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg70gEGNzZqMGo0qAIAsAIA&sourceid=chrome&ie=UTF-8",
        "type": "article"
      }
    ]
  },
  "vRS7DW2WUaXiHk9oJgg3z": {
    "title": "K-Fold Cross Validation",
    "description": "K-Fold Cross Validation is a technique used to assess how well a machine learning model will generalize to an independent dataset. It works by dividing the available data into _k_ equally sized folds or subsets. The model is then trained _k_ times, each time using _k-1_ folds as the training set and the remaining fold as the validation set. The performance metrics from each of the _k_ iterations are then averaged to provide an overall estimate of the model's performance.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Cross-validation: evaluating estimator performance | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/cross_validation.html",
        "type": "article"
      },
      {
        "title": "A Comprehensive Guide to K-Fold Cross Validation",
        "url": "https://www.datacamp.com/tutorial/k-fold-cross-validation",
        "type": "article"
      },
      {
        "title": "Complete Guide to Cross Validation",
        "url": "https://www.youtube.com/watch?v=-8s9KuNo5SA&t=925s",
        "type": "video"
      }
    ]
  },
  "_Z2miSW4PwILMRtBFajBn": {
    "title": "Deep Learning Architectures",
    "description": "Deep learning architectures are the specific arrangements of layers within a neural network that define how data is processed and transformed. These architectures consist of interconnected nodes (neurons) organized in layers, where each layer performs a specific computation. Different architectures are designed to excel at different tasks, such as image recognition, natural language processing, or time series analysis, by employing unique connection patterns, layer types, and activation functions.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is a neural network?",
        "url": "https://www.cloudflare.com/en-gb/learning/ai/what-is-neural-network/",
        "type": "article"
      }
    ]
  },
  "BtO2wH7YYqE25HShI6sd9": {
    "title": "Convolutional Neural Network",
    "description": "Convolutional Neural Networks (CNNs) are a specialized type of artificial neural network primarily used for processing data that has a grid-like topology, such as images. They employ convolutional layers to automatically and adaptively learn spatial hierarchies of features from the input data. These layers use filters (or kernels) that slide across the input, performing element-wise multiplication and summation to detect patterns. Pooling layers are often used to reduce the spatial dimensions of the feature maps, decreasing computational complexity and making the network more robust to variations in the input.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What are Convolutional Neural Networks?",
        "url": "https://www.ibm.com/think/topics/convolutional-neural-networks",
        "type": "article"
      },
      {
        "title": "An Introduction to Convolutional Neural Networks (CNNs)",
        "url": "https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns",
        "type": "article"
      },
      {
        "title": "Hot Dog or Not Hot Dog â Convolutional Neural Network Course for Beginners",
        "url": "https://www.youtube.com/watch?v=nVhau51w6dM",
        "type": "video"
      }
    ]
  },
  "I-GEE7PvpQmhQSfZmxqwA": {
    "title": "Pooling",
    "description": "Pooling is a downsampling technique used in convolutional neural networks (CNNs) to reduce the spatial dimensions of feature maps. It summarizes the features present in a region of the feature map into a single value. This helps to reduce the computational cost, control overfitting, and make the network more robust to variations in the input, such as small shifts or distortions.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "A Gentle Introduction to Pooling Layers for Convolutional Neural Networks",
        "url": "https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/",
        "type": "article"
      },
      {
        "title": "Understanding CNN | Pooling in CNNN",
        "url": "https://www.youtube.com/watch?v=azRi6Bz7yc0",
        "type": "video"
      }
    ]
  },
  "a2PGTDnXKp759vFZzkjSF": {
    "title": "Padding",
    "description": "Padding, in the context of convolutional neural networks, refers to adding extra layers of \"pixels\" or values around the input image or feature map. This is typically done with zeros (zero-padding), but other values can be used. The primary purpose of padding is to control the spatial size of the output feature maps and to manage boundary effects that arise during convolution operations. By strategically adding padding, we can preserve the original input size, prevent information loss at the edges, and improve the performance of the network.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Padding In Convolutional Neural Networks",
        "url": "https://www.digitalocean.com/community/tutorials/padding-in-convolutional-neural-networks",
        "type": "article"
      },
      {
        "title": "Convolution padding and stride",
        "url": "https://www.youtube.com/watch?v=oDAPkZ53zKk",
        "type": "video"
      }
    ]
  },
  "MtoYStcZBduLSbjRuPjq0": {
    "title": "Convolution",
    "description": "Convolution is a mathematical operation that involves sliding a filter (also known as a kernel) over an input image or feature map. At each location, the filter performs element-wise multiplication with the corresponding part of the input, and then sums the results. This sum becomes a single value in the output feature map. By sliding the filter across the entire input, the convolution operation extracts features and patterns present in the image, such as edges, textures, or shapes.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Convolutional Neural Networks cheatsheet",
        "url": "https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks",
        "type": "article"
      },
      {
        "title": "But what is a convolution?",
        "url": "https://www.youtube.com/watch?v=KuXjwB4LzSA",
        "type": "video"
      }
    ]
  },
  "YWxSI45e5K_4YOrvmh6LV": {
    "title": "Strides",
    "description": "In convolutional neural networks, a stride determines how many pixels the filter (or kernel) shifts horizontally and vertically during the convolution operation. A stride of 1 means the filter moves one pixel at a time, resulting in a more detailed feature map. A larger stride, like 2 or 3, causes the filter to jump over pixels, producing a smaller feature map and reducing computational cost, but potentially missing finer details in the input image.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Padding and Stride",
        "url": "https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html",
        "type": "article"
      },
      {
        "title": "Convolution padding and stride",
        "url": "https://www.youtube.com/watch?v=oDAPkZ53zKk",
        "type": "video"
      }
    ]
  },
  "gCGHtxqD4V_Ite_AXMspf": {
    "title": "Applications of CNNs",
    "description": "CNNs have revolutionized the field of computer vision, leading to significant advancements in many real-world applications. Thanks to their power to solve complex problems like image classification, object detection, and facial recognition, CNNs power many applications we use daily, like automatically tagging friends in photos, enabling self-driving cars to \"see\" and understand their surroundings, and helping doctors diagnose diseases from medical scans. They are also used to generate realistic images, translate languages, and even create art.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Top 10 Real-World Applications of Convolutional Neural Networks",
        "url": "https://thedatascientist.com/top-10-real-world-applications-of-convolutional-neural-networks-in-2025/",
        "type": "article"
      }
    ]
  },
  "E4k6WgNXdnNoApR675VKb": {
    "title": "Image Classification",
    "description": "Convolutional Neural Networks (CNNs) are commonly used for image classification tasks. They work by automatically learning relevant features from images through convolutional layers, which detect patterns like edges and textures. These learned features are then used to classify the image into different categories, such as identifying objects like cats, dogs, or cars. CNNs excel at this because they can handle the high dimensionality of image data and are robust to variations in object position, scale, and lighting.",
    "links": []
  },
  "iSX9YExs1gS4L2CBQux5w": {
    "title": "Image Segmentation",
    "description": "Image segmentation involves dividing an image into multiple regions or segments, often to identify objects or boundaries. Convolutional Neural Networks (CNNs) are widely used for this task. They learn spatial hierarchies of features from images, enabling pixel-wise classification. This means each pixel is assigned a label indicating which segment it belongs to. For example, in a self-driving car, CNNs can segment images to identify roads, pedestrians, and other vehicles, allowing the car to \"understand\" its surroundings. Other applications include medical image analysis for tumor detection and satellite imagery analysis for land cover classification.",
    "links": []
  },
  "jPvZdgye7cBf0bPMVGf7a": {
    "title": "Image & Video Recognition",
    "description": "Convolutional Neural Networks (CNNs) are a specialized type of neural network particularly effective at processing images and videos. In image recognition, CNNs can identify objects, scenes, and faces by learning spatial hierarchies of features from pixel data. For video recognition, CNNs analyze sequences of frames to understand actions, events, and even predict future occurrences. This is achieved by extracting relevant spatial and temporal features, enabling applications like video surveillance, autonomous driving, and content analysis.",
    "links": []
  },
  "_eKuBhCCwUHnEGwHNQY-g": {
    "title": "Recommendation Systems",
    "description": "Convolutional Neural Networks (CNNs), typically used for image processing, can also enhance recommendation systems. They do this by extracting features from user-item interaction data, like purchase history or ratings. For example, a CNN can analyze a matrix representing user preferences for different movie genres to identify patterns. These patterns help predict what movies a user might enjoy, even if they haven't explicitly rated them. By learning these complex relationships, CNNs can provide more personalized and accurate recommendations.",
    "links": []
  },
  "H0cscBaExZPNZuFubBUv7": {
    "title": "Recurrent Neural Networks",
    "description": "Recurrent Neural Networks (RNNs) are a type of neural network designed to process sequential data, where the order of the inputs matters. Unlike traditional feedforward networks that treat each input independently, RNNs have a \"memory\" that allows them to consider previous inputs when processing new ones. This memory is implemented through recurrent connections, which feed the output of a neuron back into itself or other neurons in the network, enabling the network to learn patterns and dependencies across time or sequence steps.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is a Recurrent Neural Network (RNN)?",
        "url": "https://www.ibm.com/think/topics/recurrent-neural-networks",
        "type": "article"
      },
      {
        "title": "What is RNN? - Recurrent Neural Networks Explained",
        "url": "https://aws.amazon.com/what-is/recurrent-neural-network/",
        "type": "article"
      },
      {
        "title": "Recurrent Neural Networks (RNNs), Clearly Explained",
        "url": "https://www.youtube.com/watch?v=AsNTP8Kwu80",
        "type": "video"
      }
    ]
  },
  "LpggrF1MMvAxtO9EJe3wY": {
    "title": "RNN",
    "description": "Recurrent Neural Networks (RNNs) are a type of neural network designed to process sequential data. Unlike standard feedforward networks that treat each input independently, RNNs have connections that loop back on themselves, allowing them to maintain a \"memory\" of past inputs. This memory enables them to learn patterns and dependencies across sequences, making them suitable for tasks like natural language processing, time series analysis, and speech recognition.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Recurrent Neural Network Tutorial (RNN)",
        "url": "https://www.datacamp.com/tutorial/tutorial-for-recurrent-neural-network",
        "type": "article"
      }
    ]
  },
  "ZWDSLqxmfg3aPBZFH479q": {
    "title": "GRU",
    "description": "A Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) architecture. It's designed to handle the vanishing gradient problem often encountered when training standard RNNs, especially with long sequences of data. GRUs use \"gates\" to control the flow of information, deciding what information to keep and what to discard at each time step. These gates are learned during training and allow the network to selectively remember or forget previous states, making it more effective at capturing long-range dependencies in sequential data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Understanding Gated Recurrent Unit (GRU) in Deep Learning",
        "url": "https://medium.com/@anishnama20/understanding-gated-recurrent-unit-gru-in-deep-learning-2e54923f3e2",
        "type": "article"
      },
      {
        "title": "GRU Recurrent Neural Networks â A Smart Way to Predict Sequences in Python",
        "url": "https://towardsdatascience.com/gru-recurrent-neural-networks-a-smart-way-to-predict-sequences-in-python-80864e4fe9f6/",
        "type": "article"
      },
      {
        "title": "Simple Explanation of GRU (Gated Recurrent Units)",
        "url": "https://www.youtube.com/watch?v=tOuXgORsXJ4",
        "type": "video"
      }
    ]
  },
  "LdUwTWfCIcowwC-e6q3ac": {
    "title": "LSMT",
    "description": "LSTMs are a special kind of recurrent neural network (RNN) architecture designed to handle the vanishing gradient problem that often occurs when training standard RNNs. They excel at processing sequential data by maintaining a \"memory\" of past inputs, allowing them to learn long-term dependencies. This memory is controlled by gates that regulate the flow of information into and out of the cell state, enabling LSTMs to selectively remember or forget information over time.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "A Gentle Introduction to Long Short-Term Memory Networks by the Experts",
        "url": "https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/",
        "type": "article"
      },
      {
        "title": "Long Short-Term Memory (LSTM), Clearly Explained",
        "url": "https://www.youtube.com/watch?v=YCzL96nL7j0",
        "type": "video"
      },
      {
        "title": "Simple Explanation of LSTM",
        "url": "https://www.youtube.com/watch?v=LfnrRPFhku",
        "type": "video"
      }
    ]
  },
  "-tzeA13f2jYDm4aO5JciT": {
    "title": "Attention Mechanisms",
    "description": "Developed by Google researchers, the attention mechanisms allow a neural network to focus on the most relevant parts of the input data when making predictions. Instead of processing the entire input uniformly, attention mechanisms assign weights to different parts of the input, indicating their importance. These weights are then used to create a weighted average of the input, which is used for further processing. This allows the model to selectively attend to the most informative parts of the input, improving performance, especially in tasks involving sequential data like text or images.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Attention Is All You Need",
        "url": "https://arxiv.org/abs/1706.03762",
        "type": "article"
      },
      {
        "title": "What is an attention mechanism?",
        "url": "https://www.ibm.com/think/topics/attention-mechanism",
        "type": "article"
      },
      {
        "title": "Attention mechanism: Overview",
        "url": "https://www.youtube.com/watch?v=fjJOgb-E41w",
        "type": "video"
      }
    ]
  },
  "kvf2CUKBe4qSbZla4Brh3": {
    "title": "Autoencoders",
    "description": "Autoencoders are a type of neural network used for unsupervised learning. They work by compressing the input data into a lower-dimensional representation (encoding) and then reconstructing the original input from this compressed representation (decoding). The network is trained to minimize the difference between the original input and the reconstructed output, forcing it to learn efficient and meaningful representations of the data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is an autoencoder? | IBM",
        "url": "https://www.ibm.com/think/topics/autoencoder",
        "type": "article"
      },
      {
        "title": "Intro to Autoencoders | TensorFLow",
        "url": "https://www.tensorflow.org/tutorials/generative/autoencoder",
        "type": "article"
      },
      {
        "title": "Autoencoders",
        "url": "https://www.youtube.com/watch?v=hZ4a4NgM3u0",
        "type": "video"
      }
    ]
  },
  "rDIg16eb6B6um1P8uMy51": {
    "title": "Transformers",
    "description": "Transformers are a type of neural network architecture that rely on attention mechanisms to weigh the importance of different parts of the input data. Unlike recurrent neural networks (RNNs) that process data sequentially, transformers can process the entire input at once, allowing for parallelization and capturing long-range dependencies more effectively. This architecture is particularly well-suited for tasks involving sequential data, such as natural language processing, where understanding the context of words within a sentence is crucial.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "LLM Course | HuggingFace",
        "url": "https://huggingface.co/learn/llm-course/chapter1/1",
        "type": "course"
      },
      {
        "title": "What is a Transformer Model?",
        "url": "https://www.ibm.com/think/topics/transformer-model",
        "type": "article"
      },
      {
        "title": "How Transformers Work: A Detailed Exploration of Transformer Architecture",
        "url": "https://www.datacamp.com/tutorial/how-transformers-work",
        "type": "article"
      },
      {
        "title": "Transformers, explained: Understand the model behind GPT, BERT, and T5",
        "url": "https://www.youtube.com/watch?v=SZorAJ4I-sA&t",
        "type": "video"
      }
    ]
  },
  "J1aGPkZqDZfUwpVmC88AL": {
    "title": "Multi-head Attention",
    "description": "Multi-head attention is an attention mechanism that runs through the attention process multiple times independently. Each of these independent attention mechanisms is called a \"head.\" The outputs of all the heads are then concatenated and linearly transformed to produce the final output. This allows the model to attend to different parts of the input sequence with different learned representations, capturing a richer set of relationships than a single attention mechanism could.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Understanding Multi-Head Attention in Transformers",
        "url": "https://www.datacamp.com/es/tutorial/multi-head-attention-transformers",
        "type": "article"
      },
      {
        "title": "Multi-head Attention",
        "url": "https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html",
        "type": "article"
      }
    ]
  },
  "oTKC1o1OOnPiTh60a8yVc": {
    "title": "Self-Attention",
    "description": "Self-attention is a mechanism that allows a model to focus on different parts of the input sequence when processing it. Instead of treating each element in the sequence independently, self-attention calculates a weighted sum of all elements, where the weights are determined by the relationships between the elements themselves. This enables the model to capture dependencies and contextual information within the input sequence, regardless of their distance from each other.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is self-attention?",
        "url": "https://www.ibm.com/think/topics/self-attention",
        "type": "article"
      },
      {
        "title": "Why is Self Attention called \"Self\"? | Self Attention Vs Luong Attention in Depth Lecture",
        "url": "https://www.youtube.com/watch?v=o4ZVA0TuDRg",
        "type": "video"
      }
    ]
  },
  "IR0wVIcu1MxOOBiLBnn8S": {
    "title": "Generative Adversarial Networks",
    "description": "Generative Adversarial Networks (GANs) are a type of neural network architecture designed to generate new data that resembles the data they were trained on. They consist of two networks: a generator, which creates new data instances, and a discriminator, which evaluates the authenticity of the generated data. These two networks are trained in an adversarial process, where the generator tries to fool the discriminator, and the discriminator tries to distinguish between real and generated data. This competition drives both networks to improve, ultimately leading the generator to produce highly realistic data.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "GANs | Google",
        "url": "https://developers.google.com/machine-learning/gan",
        "type": "course"
      },
      {
        "title": "What is a GAN? | AWS",
        "url": "https://aws.amazon.com/what-is/gan/",
        "type": "article"
      },
      {
        "title": "Generative Adversarial Networks | HuggingFace",
        "url": "https://huggingface.co/learn/computer-vision-course/en/unit5/generative-models/gans",
        "type": "article"
      },
      {
        "title": "What are GANs (Generative Adversarial Networks)?",
        "url": "https://www.youtube.com/watch?v=TpMIssRdhco",
        "type": "video"
      }
    ]
  },
  "JVXe2QDQaqiJYPupIMhWe": {
    "title": "Natural Language Processing",
    "description": "Natural Language Processing (NLP) is a field focused on enabling computers to understand, interpret, and generate human language. It bridges the gap between human communication and computer understanding by developing algorithms and models that can process and analyze text and speech data. This allows machines to perform tasks like translation, sentiment analysis, and text summarization.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Natural Language Processing with Python",
        "url": "https://tjzhifei.github.io/resources/NLTK.pdf",
        "type": "article"
      },
      {
        "title": "Natural Language Processing",
        "url": "https://www.deeplearning.ai/resources/natural-language-processing/",
        "type": "article"
      },
      {
        "title": "What is Natural Language Processing (NLP)? | AWS",
        "url": "https://aws.amazon.com/what-is/nlp/",
        "type": "article"
      },
      {
        "title": "Stanfordâs Natural Language Processing with Deep Learning",
        "url": "https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4",
        "type": "video"
      }
    ]
  },
  "QbftToskhtBlTz1jyiRkb": {
    "title": "Tokenization",
    "description": "Tokenization is the process of breaking down a text string into smaller units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements. The goal is to convert raw text into a format that can be easily processed and analyzed by a computer.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Tokenization? Types, Use Cases, Implementation",
        "url": "https://www.datacamp.com/blog/what-is-tokenization",
        "type": "article"
      },
      {
        "title": "The Art of Tokenization: Breaking Down Text for AI",
        "url": "https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25/",
        "type": "article"
      }
    ]
  },
  "pP0VeUSK9CDodgz-BQmrP": {
    "title": "Lemmatization",
    "description": "Lemmatization is a text normalization technique in natural language processing used to reduce words to their dictionary form, known as a lemma. Unlike stemming, which simply chops off prefixes or suffixes, lemmatization considers the context of the word and applies morphological analysis to find the base or dictionary form. This ensures that the resulting lemma is a valid word, providing a more accurate and meaningful representation of the original word.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What Are Stemming and Lemmatization?",
        "url": "https://www.ibm.com/think/topics/stemming-lemmatization",
        "type": "article"
      },
      {
        "title": "Stemming, Lemmatization- Which One is Worth Going For?",
        "url": "https://towardsdatascience.com/stemming-lemmatization-which-one-is-worth-going-for-77e6ec01ad9c/",
        "type": "article"
      }
    ]
  },
  "UO1GEUe8e22uRB6DAxfpe": {
    "title": "Stemming",
    "description": "Stemming is a text normalization technique in natural language processing that reduces words to their root or base form, known as the stem. This is achieved by removing suffixes (like \"-ing\", \"-ed\", \"-s\") from words. The goal is to treat words with similar meanings as the same, even if they have slightly different forms, which helps to simplify text analysis and improve the efficiency of certain NLP tasks.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What are stemming and lemmatization?",
        "url": "https://www.ibm.com/think/topics/stemming-lemmatization",
        "type": "article"
      }
    ]
  },
  "CaHbAXDIJQXcQ9DZqziod": {
    "title": "Embeddings",
    "description": "Embeddings are a way to represent words, phrases, or even entire documents as numerical vectors in a high-dimensional space. The goal is to capture the semantic meaning of the text, so that words with similar meanings are located close to each other in the vector space. This allows machine learning models to understand relationships between words and perform tasks like text classification, sentiment analysis, and machine translation more effectively.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Getting Started With Embeddings",
        "url": "https://huggingface.co/blog/getting-started-with-embeddings",
        "type": "article"
      },
      {
        "title": "A Guide on Word Embeddings in NLP",
        "url": "https://www.turing.com/kb/guide-on-word-embeddings-in-nlp",
        "type": "article"
      }
    ]
  },
  "sChxcuQ2OruKVx8P4wAK_": {
    "title": "Attention Models",
    "description": "Attention models in natural language processing allow a neural network to focus on specific parts of the input sequence when producing an output. Instead of relying on a fixed-length vector representation of the entire input, these models learn to assign weights to different input elements, indicating their relevance to the current output. This mechanism enables the model to selectively attend to the most important information, improving performance in tasks like machine translation and text summarization.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is an attention mechanism?",
        "url": "https://www.ibm.com/think/topics/attention-mechanism",
        "type": "article"
      }
    ]
  },
  "Tv3sZvus76dmu0X9AqCIU": {
    "title": "Explainable AI",
    "description": "Explainable AI (XAI) focuses on making machine learning models and their decisions understandable to humans. Instead of treating models as black boxes, XAI aims to provide insights into how a model arrives at a particular prediction or decision. This involves developing techniques that allow us to interpret the model's internal logic, identify the factors that influence its outputs, and assess its reliability and potential biases.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "What is Explainable AI (XAI)?",
        "url": "https://www.ibm.com/think/topics/explainable-ai",
        "type": "article"
      },
      {
        "title": "Explainable AI (XAI) | Giskard",
        "url": "https://www.giskard.ai/glossary/explainable-ai-xai",
        "type": "article"
      },
      {
        "title": "Explainable AI: Demystifying AI Agents Decision-Making",
        "url": "https://www.youtube.com/watch?v=yJkCuEu3K68",
        "type": "video"
      }
    ]
  },
  "5xxAg18h74pDAUPy6P8NQ": {
    "title": "Train - Test Data",
    "description": "When building a machine learning model, we usually split our dataset into two parts: a training set and a testing set. The training set is used to teach the model how to make predictions, while the testing set is used to evaluate how well the model has learned. This helps us understand if the model can generalize to new, unseen data. In scikit-learn, you can easily split your data using the `train_test_split` function from the `model_selection` module. You provide your data and labels to this function, and it returns the split datasets. You can also specify the proportion of data to be used for testing.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "train_test_split",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html",
        "type": "article"
      },
      {
        "title": "Split Your Dataset With scikit-learn's train_test_split()",
        "url": "https://realpython.com/train-test-split-python-data/",
        "type": "article"
      },
      {
        "title": "Train Test Split with Python Machine Learning (Scikit-Learn)",
        "url": "https://www.youtube.com/watch?v=SjOfbbfI2qY",
        "type": "video"
      }
    ]
  },
  "-W2uAccH7Y2XIwhfl9mDF": {
    "title": "Data Preparation",
    "description": "Scikit-learn provides tools to get your data ready for machine learning models. This often involves cleaning, transforming, and scaling your data. Cleaning might mean handling missing values using techniques like imputation. Transformation can involve converting categorical features into numerical ones using methods like one-hot encoding. Scaling ensures that all features contribute equally to the model by bringing them to a similar range, using techniques like standardization or normalization. These steps help improve the performance and accuracy of your machine learning models.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Preprocessing data",
        "url": "https://scikit-learn.org/stable/modules/preprocessing.html",
        "type": "article"
      },
      {
        "title": "Scikit-learn Crash Course - Machine Learning Library for Python",
        "url": "https://www.youtube.com/watch?v=0B5eIE_1vpU&t=188s",
        "type": "video"
      }
    ]
  },
  "bIhGv4886V4RWJD3tX0a0": {
    "title": "Data Loading",
    "description": "Data loading in Scikit-learn refers to the process of importing datasets into a format that can be used for machine learning tasks. This involves reading data from various sources, such as CSV files, databases, or even directly from NumPy arrays, and structuring it into a format that Scikit-learn's algorithms can understand, typically NumPy arrays or Pandas DataFrames. The loaded data is then usually split into features (independent variables) and a target variable (dependent variable) for training and evaluating machine learning models.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Dataset loading utilities",
        "url": "https://scikit-learn.org/stable/datasets.html",
        "type": "article"
      },
      {
        "title": "Scikit-Learn Full Crash Course - Python Machine Learning",
        "url": "https://www.youtube.com/watch?v=SIEaLBXr0rk",
        "type": "video"
      }
    ]
  },
  "m4vmnxRMBf7zwNnwrMEnk": {
    "title": "Tuning",
    "description": "Scikit-learn provides tools to find the best settings (hyperparameters) for your machine learning models. Instead of manually trying different values, you can use techniques like GridSearchCV or RandomizedSearchCV. These methods systematically test a range of hyperparameter combinations using cross-validation to evaluate performance. The goal is to identify the hyperparameter set that yields the best model performance on your data, improving accuracy and generalization.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Tuning the hyper-parameters of an estimator",
        "url": "https://scikit-learn.org/stable/modules/grid_search.html",
        "type": "article"
      },
      {
        "title": "A Comprehensive Guide to Cross-Validation with Scikit-Learn and Python",
        "url": "https://www.youtube.com/watch?v=glLNo1ZnmPA",
        "type": "video"
      },
      {
        "title": "Hands-On Hyperparameter Tuning with Scikit-Learn: Tips and Tricks",
        "url": "https://www.youtube.com/watch?v=LrCylIe0RJM",
        "type": "video"
      }
    ]
  },
  "vBnqnIh_xSn0OuY9oQ5e-": {
    "title": "Prediction",
    "description": "In scikit-learn, prediction means using a trained machine learning model to estimate an output (or target variable) for new, unseen data. After a model is trained on a dataset, it learns the relationship between the input features and the target variable. To make a prediction, you provide the model with new input features, and it uses the learned relationship to generate a predicted value for the target. This is typically done using the `.predict()` method on a trained model object, which takes the new data as input and returns the model's predictions.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Metrics and scoring: quantifying the quality of predictions",
        "url": "https://scikit-learn.org/stable/modules/model_evaluation.html",
        "type": "article"
      },
      {
        "title": "How to Make Predictions with scikit-learn",
        "url": "https://machinelearningmastery.com/make-predictions-scikit-learn/",
        "type": "article"
      }
    ]
  },
  "Ddhph9saFgfMi-uUFGK75": {
    "title": "Model Selection",
    "description": "Model selection is the process of choosing the best machine learning model from a set of candidate models for a given task. Scikit-learn offers a wide range of models, including linear models (like linear regression and logistic regression), tree-based models (like decision trees and random forests), support vector machines (SVMs), and neural networks, enabling you to find the most suitable model for your specific problem.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Supervised Learning Models",
        "url": "https://scikit-learn.org/stable/supervised_learning.html",
        "type": "article"
      },
      {
        "title": "Unsupervised Learning Models",
        "url": "https://scikit-learn.org/stable/unsupervised_learning.html",
        "type": "article"
      }
    ]
  },
  "A_Kx3pEj0jpnLJzdOpcQ9": {
    "title": "Deep Learning Libraries",
    "description": "Deep learning relies heavily on specialized Python libraries that provide pre-built functions and tools for building and training neural networks. TensorFlow, developed by Google, is a widely used library known for its flexibility and scalability. PyTorch, created by Facebook, is another popular choice, favored for its dynamic computation graph and ease of use, especially in research. Keras acts as a high-level API that can run on top of TensorFlow or other backends, simplifying the process of building complex models. These libraries offer functionalities like automatic differentiation, GPU acceleration, and pre-trained models, making deep learning more accessible and efficient.",
    "links": []
  },
  "Ru8_xMyFxye1hyzCUYvnj": {
    "title": "TensorFlow",
    "description": "TensorFlow is an open-source software library created by Google for numerical computation and large-scale machine learning. It provides a flexible architecture and tools that allow users to easily build and deploy machine learning models, particularly deep neural networks. TensorFlow excels at handling complex computations across various platforms, including CPUs, GPUs, and TPUs, making it suitable for research, development, and production environments.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "TensorFlow",
        "url": "https://www.tensorflow.org/",
        "type": "article"
      },
      {
        "title": "TensorFlow tutorials",
        "url": "https://www.tensorflow.org/tutorials",
        "type": "article"
      },
      {
        "title": "Mastering Deep Learning with TensorFlow: From Beginner to Expert",
        "url": "https://towardsdatascience.com/an-introduction-to-tensorflow-fa5b17051f6b/",
        "type": "article"
      },
      {
        "title": "Python TensorFlow for Machine Learning â Neural Network Text Classification Tutorial",
        "url": "https://www.youtube.com/watch?v=VtRLrQ3Ev-U",
        "type": "video"
      }
    ]
  },
  "zoXnXI4Wf5sxddHqYwQjP": {
    "title": "Keras",
    "description": "Keras is a high-level, user-friendly neural networks API written in Python. It acts as an interface for several lower-level backends like TensorFlow, CNTK, and Theano. This allows developers to quickly prototype and build deep learning models without needing to delve into the complexities of the underlying computational frameworks. Keras focuses on enabling fast experimentation through its modularity, ease of use, and support for various neural network layers and optimizers.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Getting Familiar with Keras",
        "url": "https://towardsdatascience.com/getting-familiar-with-keras-dd17a110652d/",
        "type": "course"
      },
      {
        "title": "Keras",
        "url": "https://github.com/keras-team/keras",
        "type": "opensource"
      },
      {
        "title": "Keras",
        "url": "https://keras.io/",
        "type": "article"
      },
      {
        "title": "Keras Crash Course | Deep Learning, Image Modelling, RNNs and More",
        "url": "https://www.youtube.com/watch?v=a8op1jBG7oM",
        "type": "article"
      }
    ]
  },
  "7RSW-Pypf3QpZp4O21AGl": {
    "title": "PyTorch",
    "description": "PyTorch is an open-source machine learning framework primarily developed by Meta AI. It's used for a variety of applications, including computer vision, natural language processing, and reinforcement learning. PyTorch is known for its dynamic computation graph, which allows for more flexibility and easier debugging compared to static graph frameworks. It provides a comprehensive set of tools and libraries to build and train neural networks.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "PyTorch",
        "url": "https://pytorch.org/",
        "type": "article"
      },
      {
        "title": "What is PyTorc? | IBM",
        "url": "https://www.ibm.com/think/topics/pytorch",
        "type": "article"
      },
      {
        "title": "PyTorch for Deep Learning & Machine Learning â Full Course",
        "url": "https://www.youtube.com/watch?v=V_xro1bcAuA",
        "type": "video"
      }
    ]
  },
  "yKtRmgwrJ75VVz7_txZ-S": {
    "title": "Scikit-learn",
    "description": "Scikit-learn is a popular Python library mainly used for traditional machine learning tasks like classification, regression, and clustering. While it's not primarily designed for deep learning, it does offer basic tools for creating simple neural networks, specifically multi-layer perceptrons (MLPs). You can use `sklearn.neural_network.MLPClassifier` for classification problems and `sklearn.neural_network.MLPRegressor` for regression problems. These tools allow you to quickly build and train basic neural networks without needing a dedicated deep learning framework like TensorFlow or PyTorch.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "Neural network models | scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html",
        "type": "article"
      },
      {
        "title": "How to train and test a neural network using scikit-learn and Keras in Jupyter Notebook.",
        "url": "https://www.youtube.com/watch?v=_JG71FIP1rk",
        "type": "video"
      }
    ]
  },
  "0-6BV-MggAyD7g3JH45B7": {
    "title": "ElasticNet Regularization",
    "description": "Elastic Net is a regularization technique that combines the penalties of both L1 (Lasso) and L2 (Ridge) regularization methods. It aims to improve model performance by addressing limitations of each individual method, particularly in situations where there are many correlated features. By using a linear combination of L1 and L2 penalties, Elastic Net can perform feature selection (like Lasso) and handle multicollinearity (like Ridge) simultaneously.\n\nVisit the following resources to learn more:",
    "links": [
      {
        "title": "ElasticNet| scikit-learn",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html",
        "type": "article"
      },
      {
        "title": "https://www.youtube.com/watch?v=xl6KAAVytEk",
        "url": "https://www.youtube.com/watch?v=xl6KAAVytEk",
        "type": "video"
      }
    ]
  },
  "vmERbhRIevLLNc7Ny2pWp": {
    "title": "Why is it important?",
    "description": "Model evaluation is the process of assessing how well a machine learning model performs on a given dataset. It involves using various metrics and techniques to quantify the model's accuracy, reliability, and generalization ability. This assessment helps determine if the model is suitable for deployment and provides insights into areas where it can be improved.",
    "links": []
  }
}