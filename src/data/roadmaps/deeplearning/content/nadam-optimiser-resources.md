# Nadam Optimiser

Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the minimum of the function.The Nesterov-accelerated Adaptive Moment Estimation, or the Nadam, algorithm is an extension to the Adaptive Movement Estimation (Adam) optimization algorithm to add Nesterov's Accelerated Gradient (NAG) or Nesterov momentum, which is an improved type of momentum.Learn about gradient descent, an optimization algorithm used to train machine learning models by minimizing errors between predicted and actual results.

- [NADAM](https://keras.io/api/optimizers/Nadam/)
- [Gradient Descent in Machine Learning](https://builtin.com/data-science/gradient-descent)
- [experimental.Nadam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/Nadam)
- [Scratch from python](https://www.youtube.com/watch?v=8nfd7gEDKCc)



