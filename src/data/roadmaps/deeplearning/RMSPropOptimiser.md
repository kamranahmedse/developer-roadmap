# What is RMSprop optimizer?

## RMSprop is a gradient based optimization technique used in training neural networks. It was proposed by the father of back-propagation, Geoffrey Hinton. Gradients of very complex functions like neural networks have a tendency to either vanish or explode as the data propagates through the function.

# Sources of RMSprop optimizer:

[RMSprop Deepcheck](https://deepchecks.com/glossary/rmsprop/)

[RMSprop DL](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/)

[RMSprop](https://optimization.cbe.cornell.edu/index.php?title=RMSProp)

[RMSprop Optimizer](https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be)

[Yacine Mahdid YB](https://www.youtube.com/watch?v=nLCuzsQaAKE)





