# Inference

In artificial intelligence (AI), inference refers to the process where a trained machine learning model makes predictions or draws conclusions from new, unseen data. Unlike training, inference involves the model applying what it has learned to make decisions without needing examples of the exact result. In essence, inference is the AI model actively functioning. For example, a self-driving car recognizing a stop sign on a road it has never encountered before demonstrates inference. The model identifies the stop sign in a new setting, using its learned knowledge to make a decision in real-time.

Visit the following resources to learn more:

- [@article@Inference vs Training](https://www.cloudflare.com/learning/ai/inference-vs-training/)
- [@article@What is Machine Learning Inference?](https://hazelcast.com/glossary/machine-learning-inference/)
- [@article@What is Machine Learning Inference? An Introduction to Inference Approaches](https://www.datacamp.com/blog/what-is-machine-learning-inference)