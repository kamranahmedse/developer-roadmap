# Prompt Hacking
Prompt hacking refers to techniques used to manipulate or exploit AI language models by carefully crafting input prompts. This practice aims to bypass the model's intended constraints or elicit unintended responses. Common methods include injection attacks, where malicious instructions are embedded within seemingly innocent prompts, and prompt leaking, which attempts to extract sensitive information from the model's training data.

- [@article@Prompt Hacking](https://learnprompting.org/docs/prompt_hacking/intro)
- [@feed@Explore top posts about Security](https://app.daily.dev/tags/security?ref=roadmapsh)
