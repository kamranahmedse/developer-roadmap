# Gradient, Jacobian, and Hessian

The gradient, Jacobian, and Hessian are fundamental tools from calculus used to analyze and optimize functions, especially in the context of machine learning. The gradient of a scalar-valued function of multiple variables is a vector containing the partial derivatives with respect to each variable, indicating the direction of the steepest ascent. The Jacobian matrix generalizes the gradient to vector-valued functions of multiple variables, containing all the partial derivatives of each output component with respect to each input variable. The Hessian matrix, on the other hand, is the square matrix of second-order partial derivatives of a scalar-valued function, providing information about the local curvature of the function.

Visit the following resources to learn more:

- [@article@Vector Calculus: Understanding the Gradient](https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/)
- [@article@A Gentle Introduction to the Jacobian](https://machinelearningmastery.com/a-gentle-introduction-to-the-jacobian/)
- [@article@A Gentle Introduction To Hessian Matrices](https://www.machinelearningmastery.com/a-gentle-introduction-to-hessian-matrices/)
- [@video@Partial Derivatives and the Gradient of a Function](https://www.youtube.com/watch?v=AXH9Xm6Rbfc&t=320s&pp=ygURZ3JhZGllbnQgY2FsY3VsdXM%3D)
- [@video@Change of Variables and the Jacobian](https://www.youtube.com/watch?v=hhFzJvaY__U)
- [@video@Multivariable Calculus: Lecture 3 Hessian Matrix : Optimization for a three variable function](https://www.youtube.com/watch?v=zomvvohLwr4)