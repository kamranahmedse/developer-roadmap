# Feature Scaling & Normalization

Feature scaling is a preprocessing technique in machine learning that transforms numerical features to a common scale, ensuring they contribute equally to the model by preventing features with larger ranges from dominating, which is particularly important for algorithms sensitive to feature scales like gradient descent-based methods. Key methods include Standardization (transforming data to a mean of 0 and a standard deviation of 1, often better for outliers), and Normalization (scaling data to a fixed range, often 0 to 1).

Visit the following resources to learn more:

- [@article@Normalization vs. Standardization: How to Know the Difference](https://www.datacamp.com/tutorial/normalization-vs-standardization)
- [@video@Standardization vs Normalization Clearly Explained!](https://www.youtube.com/watch?v=sxEqtjLC0aM)