# Tokenization

Tokenization is the process of breaking down a text string into smaller units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements. The goal is to convert raw text into a format that can be easily processed and analyzed by a computer.

Visit the following resources to learn more:

- [@article@What is Tokenization? Types, Use Cases, Implementation](https://www.datacamp.com/blog/what-is-tokenization)
- [@article@The Art of Tokenization: Breaking Down Text for AI](https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25/)