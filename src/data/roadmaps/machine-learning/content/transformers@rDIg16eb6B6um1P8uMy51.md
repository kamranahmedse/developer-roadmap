# Transformers

Transformers are a type of neural network architecture that rely on attention mechanisms to weigh the importance of different parts of the input data. Unlike recurrent neural networks (RNNs) that process data sequentially, transformers can process the entire input at once, allowing for parallelization and capturing long-range dependencies more effectively. This architecture is particularly well-suited for tasks involving sequential data, such as natural language processing, where understanding the context of words within a sentence is crucial.

Visit the following resources to learn more:

- [@article@What is a Transformer Model?](https://www.ibm.com/think/topics/transformer-model)
- [@article@How Transformers Work: A Detailed Exploration of Transformer Architecture](https://www.datacamp.com/tutorial/how-transformers-work)
- [@course@LLM Course | HuggingFace](https://huggingface.co/learn/llm-course/chapter1/1)
- [@video@Transformers, explained: Understand the model behind GPT, BERT, and T5](https://www.youtube.com/watch?v=SZorAJ4I-sA&t)