# Log Loss

Log Loss, also known as cross-entropy loss, quantifies the performance of a classification model where the prediction input is a probability value between 0 and 1. It measures the uncertainty of the model's predicted probabilities compared to the actual labels. Lower Log Loss values indicate better calibrated predictions, meaning the predicted probabilities align more closely with the true outcomes.

Visit the following resources to learn more:

- [@article@log_loss | scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)
- [@article@Intuition behind Log-loss Score](https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a/)